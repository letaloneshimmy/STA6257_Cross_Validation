---
title: "Cross Validation"
subtitle: "Resampling methods to evaluate model fit"
author: "Sai Devarasheyyt, Mattick, Musson, Perez"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Week 2 Atrical Summaries

### Atrical-1 [On Estimating Model Accuracy with Repeated Cross-Validation](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C10&q=On+Estimating+Model+Accuracy+with+Repeated+Cross-Validation.&btnG=) [@vanwinckelen2012estimating]


#### Goal/why
The authors objective is to evaluate the benefits of estimating model accuracy with Repeated Cross-Validation (CV). Repeated CV is the process of repeat the CV multiple times then calculating a mean and confidence interval around the mean. Their hypothesis is that repeated CV is not useful.

#### Methods
A large data set was used to represent a population P
Nine data sets S were created by randomly sampling P n times.

-       Experiment 1: The C4.5 method was used to model each S. S = 9, n = 200
-       Experiment 2: The Naive Bayes method was used to model each S. S = 9, n = 200
-       Experiment 3: The C4.5 method was used to model each S. S = 9, n = 1000
-       Experiment 4: The Naive Bayes method was used to model each S. S = 9, n = 1000

Three 10-Fold CV estimates were calculated for each S. Each of the three CV estimates were repeated at different frequencies (1x, 10x, 30x) 
The mean CV estimate was calculated and a 95% CI was constructed around the mean. 
The population accuracy for each model was calculated.
The mean CV estimate and its upper and lower CI estimates were compared to the population accuracy.

#### results
The 95%CI range decreases as the number of CV repetitions increases. 
The model population accuracy value was within most (29/36) of the 1xCV 95%CI. As the number of CV reps. increase fewer of the model population accuracy value were within the 95%CI.
When the model population accuracy value was outside the 95%CI, most of the model population accuracy value were to the right of the 95%CI. Showing a pessimistic bias. 
The pessimistic bias decreases with an increased sample size.

#### limitations
According to the author repeated CV is recommend to reduce variance. Other CV studies have concluded that variance is greatly reduced when using the 10-fold CV, when compared to the Leave-One-Out CV method.  This experiment was done using the 10-fold CV method. I wounder if this experiment were done using LOOCV if the results/conclusions would be the same/similar.

If I  used Repetitive CV to estimate model error, I would not use the mean CV value, I would use the max CV error estimate. 

#### data that was used
Frank, A. and Asuncion, A. (2010) UCI Machine Learning Repository. University of California, School of Information and Computer Science. Irvine. http://archive.ics.uci.edu/ ml



### Atrical-2 [Assessing Model Fit by Cross-Validation](https://pubs.acs.org/doi/pdf/10.1021/ci025626i?casa_token=iSpUkNhlfsMAAAAA:nzsLXNNwrbIqqpp4fR5o8nYDSRt2Jr7SCMFXFA-n0lsb7G55C3jlDrTEIeJG89MYjxlQ-nmz2Z7ScepT) [@hawkins2003assessing]


### Goal

Provide evidence that, when the sample is small ("dozens or scores rather than the hundreds"), it is better to construct a QSAR model with all available data and use Cross-validation to estimate model error. Verses constructing a QSAR model with a portion of the available data (training set) and using the remaining portion of the data (test set) to evaluate model performance. The authors refer to the later method as "sample splitting".

### methods
#### Data cleaning 
predictors were removing if they:

-	perfectly correlated with another predictor

-	showed no compound to compound variation

-	were available for "a few" compounds

Of the original 379 predictors, 232 were used in the experiment.

#### Calibration
##### Training Set pool
100 of the 469 compounds were randomly selected.
300 ridge regression models were fit to the 100 selected compounds that predicted vapor pressure (y).

-	75 models were constructed with 5  randomly selected predictors (x)

-	75 models were constructed with 10 randomly selected predictors (x)

-	75 models were constructed with 20 randomly selected predictors (x) 

-	75 models were constructed with 50 randomly selected predictors (x) 

##### Assessment
Of the remaining 369 (469-100 training compounds) compounds, 50 were randomly selected for the test set. 
The experimental data set was the 319 remaining compounds 

The 300 models were used to predict the vapor pressure of the compounds in the experimental data set. The R-Squared [1-(sum(yi - yhat)^2 / sum(yi - ybar)^2 )] value for each compound was calculated. The experimenters made the assumption that this R-Squared value was the "True" R-Squared value of the population. 

Four experimental R-Squared values for each of the 300 calibration models were calculated to evaluate their performance.

1. Leave One Out Cross validation

2. Hold Out Set n = 10

3. Hold Out Set n = 20

4. Hold Out Set n = 50

The "true" R-Squared value mean and standard deviation were compared to the experimental R-Squared means and standard deviations. 

### results
With a sample size of 100:

-	The CV and 50-compond holdout set R-Squared values "seemed" to reliable predict the "true" R-Squared.

-	The 20-compond holdout R-Squared values showed more variation than  the CV and 50-compond holdout set R-Squared. The author concluded this method was inferior to CV and the 50-holdout. 

-	The 10-compund holdout R-Squared values showed "enormous" variation. The author concluded this method "useless".


### limitations
The authors conclusions are opinion based. This statistic is smallest than this statistic. This plot looks better that this plot. The is no evidence (hypothesis testing) to support the authors conclusions. 

### data that was used

The data set contains 469 compounds and 379 chemical descriptors. One of the descriptors was vapor pressure. Vapor pressure was the response variable in the models.

The data sets were from:

Basak, S. C.; Gute, B. D.; Grunwald, G. D. Use of topostructural
topochemical, and geometric parameters in the prediction of vapor
pressure: A hierarchical approach. J. Chem. Inf. Comput. Sci. 1997,
37, 651-655.

Basak, S. C.; Gute, B. D.; Grunwald, G. D. Use of topostructural
topochemical, and geometric parameters in the prediction of vapor
pressure: A hierarchical approach. J. Chem. Inf. Comput. Sci. 1997,
37, 651-655.




## Push, Pulbish changes to Github
  - Git, next to Environment and History
  - select all files, check box
  - commit
  - add comment
  - push
  - Github username
  - Password is the Git token


## Links
  - [Github: Andrew Mattick](https://github.com/andyMattick/STA6257_Project_CrossValidation.git)
  - [Github: Curtis Musson](https://github.com/letaloneshimmy/STA6257_Musson_CrossValidation.git)
  - [Discord: Crossvalisdation Board](https://discord.com/channels/1253401001113817270/1253401001545957406)
  - [Quarto: Markdown Basics](https://quarto.org/docs/authoring/markdown-basics.html)
  - [Quarto: Typesetting](https://www.albany.edu/spatial/talk/quarto/lecture/91-quarto-markdown.html#:~:text=Math%20symbols%20in%20Quarto/markdown%20are%20handled%20with,the%20most%20common%20system%20for%20typesetting%20mathematical)

 [website](https://letaloneshimmy.github.io/STA6257_Cross_Validation/)\
 [Slides]()


## Introduction
Cross validation is a resampling method. Resampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model [@james2013introduction].


resampling sample w/out replacement.



## Methods

 - Validation Set Approach	k%/1-k%   50/50
 - Leave-One-Out Cross-Validation k=1
 - k-fold k=n


![Leave One Out CV](LOOCV_fig.png)



![5-fold CV](CV5Fold_fig.png)





### Assumptions

### Equations


##### General Linear Model


$$
y = \beta_0 + \beta_1 X_1 + ... +\beta_k X_k +\varepsilon_i\tag{3}
$$

##### Square Error

$$
Square\ Error_i = (y_i - \hat{y}_i)^2\tag{3}
$$
  One of the major issues in MLR is finding the appropriate approach to variable selection. The benefit of adding additional variables to an MLR model is to account for or explain more of the variance of the response variable. The cost of adding additional variables is that the degrees of freedom, n−k−1, decreases (the number of independent pieces of information in the sample), making it more difficult to find significance in hypothesis tests and increasing the width of confidence intervals. A good model will explain as much of the variance of y as possible with a small number of explanatory variables.

  The first step is to consider only explanatory variables that ought to have some effect on the dependent variable. There must be plausible theory behind why a variable might be expected to influence the magnitude of y. Simply minimizing the SSE  or maximizing R2 are not sufficient criteria. In fact, any explanatory variable will reduce the SSE and increase the R2 by some small amount, even those irrelevant to the situation (or even random numbers). The benefit of adding these unrelated variables, however, is small compared to the cost of a degree of freedom. Therefore, the choice of whether to add a variable is based on a cost-benefit analysis, and variables enter the model only if they make a significant improvement in the model, because there is a loss of statistical power as more variables are added.  [@helsel1993statistical]
  
It should be noted, that it is usually advisable to choose a less complex model in order to achieve good results with the desired small sample sizes since more complex models usually require larger data sets to be sufficiently accurate. [@bischl2012resampling]


##### Mean Square Error

$$
MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i -\hat{y}_i)^2  \tag{3}
$$



##### Leave-One_Out Cross validation

$$
CV_{(n)} = \frac{1}{n}\sum_{i=1}^{n} MSE_i  \tag{3}
$$



[@james2013introduction]


#### leverage statistic

$$
h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\tag{3}
$$
[@james2013introduction] pdf page 110, book page99


#### Leave-One-Out Cross-Validation for linear models fit by least squares

LOOCV is computationally expensive and may take a lot of time to compute when n is large because the models has to be fit n times. For least squares linear or polynomial regression there is a less computationally expensive method. The following formula produces the same results as LOOCV. Where hat{y_i} is the ith fitted value nd hi is the leverage defined (99)

$$
CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{y_i - \hat{y}}{1-h_i}\right)^2\tag{3}
$$

#### k-Fold Cross-Validation


$$
CV_{(k)} = \frac{1}{k}\sum_{i=1}^{k} MSEi
$$


## Analysis and Results

### Packages
```{r, warning=FALSE, echo=TRUE}
# loading packages 
library(dplyr)
library(readr)
library(knitr)
library(ggplot2)
```

### Data


concrete data [@misc_concrete_compressive_strength_165]

#### Source


| Name                          | Data Type    | Measurement  | Variable     |
|:----------------------------- |:-------------|:-------------|:----------------|
| Concrete compressive strength | Quantitative | MPa          | Response  |
| Cement              | Quantitative | kg in a m3 mixture | Predictor |
| Blast Furnace Slag  | Quantitative | kg in a m3 mixture | Predictor |
| Fly Ash             | Quantitative | kg in a m3 mixture | Predictor |
| Water               | Quantitative | kg in a m3 mixture | Predictor |
| Superplasticizer    | Quantitative | kg in a m3 mixture | Predictor |
| Coarse Aggregate    | Quantitative | kg in a m3 mixture | Predictor |
| Fine Aggregate      | Quantitative | kg in a m3 mixture | Predictor |
| Age                 | Quantitative | Day (1~365)        | Predictor |

: Variables {.striped .hover}


#### Extract transform and Load Data

```{r, warning=FALSE, echo=TRUE}
# Load Data
concreet <- read_csv("Concrete_Data.csv") %>% 
  rename(cement = 1, 
         furnac_slag = 2,
         fly_ash = 3,
         water = 4,
         superplasticizer = 5,
         coarse_aggregate = 6,
         fine_aggregate = 7,
         age = 8,
         compressive_strength = 9 ) %>% 
  relocate(compressive_strength)
```

#### Visualize Data

```{r}
boxplot(concreet$compressive_strength)

plot1 <- concreet %>% ggplot(aes(y = compressive_strength, x =furnac_slag)) +
  geom_point() +
  geom_smooth(formula = "y~x", method=lm,se = F) 
plot1
```

### Statistical Modeling

#### Assumptions


### glm

```{r, warning=FALSE, echo=T, message=FALSE}

glm_mod <- glm(compressive_strength ~ cement + furnac_slag + 
                fly_ash + water + superplasticizer + 
                coarse_aggregate + fine_aggregate + age,
               data = concreet, 
               family = "gaussian")

# significant of regression line
full_mod <- glm_mod 
reduced_mod <- glm(compressive_strength ~ 1, data = concreet, family = "gaussian")
anova(reduced_mod, full_mod, test = "F")

# significant predictors
summary(glm_mod)

```


### Cross Validation

 - 50/50 ... 70/30 ... 90/10 ... etc
 - k-fold
 - Leave One out


## Conclusion


____________________________________________________________________________________


_________________________________________________________________________________

### What is "mehtod"?

This is an introduction to LASSO regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function


This is my work and I want to add more work...

### Related work

This section is going to cover the literature review...

## Methods


<!-- The common non-parametric regression model is -->
<!-- $Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of -->
<!-- the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown -->
<!-- and $\varepsilon_i$ some errors. With the help of this definition, we -->
<!-- can create the estimation for local averaging i.e. $m(x)$ can be -->
<!-- estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In -->
<!-- other words, this means that we are discovering the line through the -->
<!-- data points with the help of surrounding data points. The estimation -->
<!-- formula is printed below [@R-base]: -->

<!-- $$ -->
<!-- M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1} -->
<!-- $$ $W_n(x)$ is the sum of weights that belongs to all real numbers. -->
<!-- Weights are positive numbers and small if $X_i$ is far from $x$. -->


## Analysis and Results

### Data and Visualization

A study was conducted to determine how...


```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
# library(dplyr)
# library(readr)
# library(knitr)
# library(ggplot2)
# library(ggthemes)
# library(ggrepel)
# library(dslabs)
```


```{r, warning=FALSE, echo=TRUE}
 

 
# kable(head(murders))
# 
# ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 
# 
#   ggplot1 + geom_point(aes(col=region), size = 4) +
#   geom_text_repel(aes(label=abb)) +
#   scale_x_log10() +
#   scale_y_log10() +
#   geom_smooth(formula = "y~x", method=lm,se = F)+
#   xlab("Populations in millions (log10 scale)") + 
#   ylab("Total number of murders (log10 scale)") +
#   ggtitle("US Gun Murders in 2010") +
#   scale_color_discrete(name = "Region")+
#       theme_bw()
  

```

### Statistical Modeling



### Conclusion

## References
