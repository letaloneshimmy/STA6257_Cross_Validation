---
title: "Evaluating Concrete Strength Model Performance"
subtitle: "Using Cross-validation Methods"
author: "Sai Devarashetty, Mattick, Musson, Perez"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: [packages.bib, references.bib]
always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
toc: true
toc-location: right
toc-expand: true
toc-depth: 2
number-sections: true
nocite: |
  @R-base, @R-caret, @R-Core, @R-corrplot, @R-dplyr, @R-ggplot2, @R-knitr, @R-lightgbm, @R-rmarkdown, @R-Matrix, @R-Metrics, @R-mlbench, @R-readr, @R-stringr, @R-tidyr
---

[Slides](slides.html)

# Introduction

Cross-validation (CV) is an important statistical method used to measure the performance and generalizability of machine learning and predictive models. 
Generalizability is define [@song2021making] as how well predictive models created from a sample fit other samples from the same population.
When using a sample to fit a predictive model it is critical to verify that the model can be generalized to the population [@hawkins2003assessing].
One of the biggest obstacles to model generalizability is overfitting. 
Overfitting occurs when a model fits the underlying patterns of the training data too well and performs poorly on unseen data [@allgaier2024practical], [@bradshaw2023guide].
The goal of CV is to ensure a model generalizes well by minimizing overfitting and optimizing the hyperparameters [@filzmoser2009repeated], [@bradshaw2023guide].\

Cross-validation is widely used in various fields, including machine learning, data mining, and bioinformatics. In machine learning, it is essential for tuning hyperparameters, selecting models, and estimating the model’s generalization error. In data mining, cross-validation helps in validating predictive models built from large datasets. Bioinformatics leverages cross-validation to validate models for predicting protein structures, gene functions, and disease classifications [@song2021making].\

There are many different variations of CV methods. All variations are similar in that they divide a dataset into subsets, training a model on all but one of the subsets, and test the data on the one remaining subset [@song2021making], [@bischl2012resampling]. Three commonly used CV methods are:

1. **k-Fold Cross-validation**\
The k-Fold method is known for its flexibility and adaptability of various data distributions and sample sizes [@browne2000cross].
Many publications including [@james2013introduction], [@gorriz2024k] describe a <i>Bias-Variance Trade-Off</i> when comparing k-Fold and Leave-one-out Cross-validation (LOOCV) methods. The computational cost, bias, and variance of these two methods differ. Despite the lower bias in LOOCV, it is recommended to use k-fold in instances where k=5 or k= 0, over LOOCV because:\
    - the **<u>computational cost</u>** is much lower,\
    - it does not show excessive **<u>bias</u>**,\
    - and it does not show excessively high **<u>variance</u>**.\
    
2. **Leave-one-out Cross-validation**\
This approach is particularly useful for smaller datasets for which separate training and testing sets are impractical [@wong2019reliable].
Iterating through all possible n−1 subset results in a performance estimate with low bias [@yates2023cross].
One drawback to this method is the high computational cost associated with fitting the model n times. 
When n is large or the model complex, LOOCV can be unfeasible due to the prohibitive computational cost [@adin2024automatic]. 
When n is small or computational cost is irrelevant, LOOCV is good at giving a thorough, low bias evaluation of model performance [@lei2020cross], [@hawkins2003assessing].
The other drawback to LOOCV is high variance. High variance in error estimation is brought on by the frequent usage of almost identical training sets [@browne2000cross].

3. **Nested Cross-validation**\
Nested cross-validation offers a more objective estimation of the model’s capacity for generalization by isolating the hyperparameter tweaking procedure from the performance assessment. Nested Cross-validation is especially helpful when choosing a model and adjusting hyperparameters [@bradshaw2023guide]. 
Recently, recommendations to use this method when n is small have become more common [@raschka2018model]. 
According to [@filzmoser2009repeated], nested cross-validation is useful for generating precise performance estimates without causing overfitting. Overfitting can happen when hyperparameters are adjusted using the same data that is used for model evaluation. This technique is particularly important in complicated modeling situations where the model’s prediction performance is greatly impacted by parameter adjustment.

## Study Objectives

The goal of this paper is to explore the methodology of cross-validation and its application in evaluating the performance of predictive models for concrete strength. Concrete strength is a crucial parameter in construction, directly impacting the safety, durability, and cost-effectiveness of structures. Accurate prediction of concrete strength allows for optimal design, better resource allocation, and improved construction practices. Traditional methods of model validation, such as holdout validation, can sometimes provide misleading performance estimates due to their reliance on a single training-validation split. Cross-validation addresses this limitation by using multiple splits, thus providing a more robust evaluation of the model. As previously mentioned, cross-validation is a statistical technique used to assess the generalizability and reliability of a model by partitioning the data into multiple subsets. This trains the model on some subsets while validating it on others. Using this process helps prevent overfitting, ensuring that the model performs well on new, unseen data, and provides a more accurate estimate of the model's performance.\
\
In this study, we apply cross-validation to a dataset containing measurements of concrete strength. We aim to demonstrate how different cross-validation techniques, such as k-fold cross-validation and leave-one-out cross-validation, can be used to evaluate the performance of predictive models. By comparing these techniques, we seek to identify the most effective method for assessing model accuracy and reliability in the context of predicting concrete strength.

# Methods

## Model Measures of Error

Measuring the quality of fit of a regression model is an important step in data modeling. 
There are several commonly used metrics used to quantify how well a model explains the data. 
By measuring the quality of fit we can select the model that makes the most accurate predictions on unseen data. Common metrics used to measure model performance are:

- **Mean Absolute Error (MAE)**

The Mean Absolute Error is a measure of error <u>magnitude</u>. The sign of the error does not matter because MAE uses the absolute value. Small MAE values or "<i>lower magnitude</i>" indicate better model fit. MAE is calculated (1) by averaging the absolute difference between the observed $(y_i)$ and predicted $\hat{f}(x_i)$ values. Where:\

  - $n$ is the number of observations,
  - $\hat{f}(x_i)$ is the prediction that the model function $\hat{f}$ gives for the ith observation,
  - $y_i$ is the observed value.
  

$$
 \text{MAE} 
 = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{f}(x_i)|  
 \tag{1}
$$


- **Root Mean Squared Error (RMSE)**

The Root Mean Squared Error (2) is also a measure of error <u>magnitude</u>. Like MAE, smaller RMSE values indicate a better model fit.
In this method the square error $(y_i - \hat{f}(x_i))^2$ values are used. Squaring the error gives more weight to the larger ones. In contrast with the MAE that uses the absolute error $|y_i - \hat{f}(x_i)|$ values, all errors are weighted equally regardless of size. Taking the square root returns the error to the same units as the response variable, making it easier to interpret.\

$$
\text{RMSE} 
= \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2}
\tag{2}
$$


- **R-squared ($R^2$)**

R-squared is the percent of the <u>variance</u> in the response variable that is explained by the predictor variable(s). Unlike MAR and RMSE, $R^2$ values range from 0 to 1, and the higher the value, the better the fit. An $R^2$ value of 0.75 indicates that 75% of the variance in the response variable can be explained by the predictor variable(s). The $R^2$ equation (3) is composed of two key parts, the Total Sum of Squares ($SS_{tot}$) and the Residual Sum of Squares ($SS_{res}$).

$$
\text{R}^2 
= \frac{SS_{tot}-SS_{res}}{SS_{tot}} 
= 1 - \frac{SS_{res}}{SS_{tot}}
= 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2}{\sum_{i=1}^{n}(y_i-\bar{f}(x_i))^2}
 \tag{3}
$$ 




    
  [@james2013introduction], [@hawkins2003assessing], [@helsel1993statistical]
 


## k-Fold Cross-validation

For the k-Fold Cross-validation (k-Fold) method, the dataset is divided into k subsets, known as "folds." Each fold contains roughly an equal number of observations. The model is trained on k−1 folds and tested on the remaining holdout fold. Every fold is used once as the test set, and this process is repeated k times [@fig-kfold].


$$
CV_{(k)} = \frac{1}{k}\sum_{i=1}^{k} \text{Measuer of Errori}_i \tag{4}
$$


**Process:**

1. **Prepare the data**\
Subset the data randomly and without replacement into k equally sized folds. 
Each fold will contain approximately n/k observations. 
For example when n=200 and k=5, then each fold will have 200/5 = 40 observations. If n=201, then one of the folds would have 41 observations, the other four folds would have 40.

2. **Split the folds into test and training sets**\
In the previous example, if you had 5 folds, we could choose the first set to be the test set and the other 4 would be the training set. It doesn’t make a difference which one you choose as all of the folds will eventually be test folds against the other 4 [@fig-kfold].

3. **Fit model to the training data**\
Take the model you are going to use for prediction and fit it to the training data. Continuing with our example, you would use the 4 training folds to fit the model. Take the fitted model you developed in step 3 and apply it to the 1 test fold. After applying it to the model, you would take the resulting prediction and determine the accuracy by comparing what was predicted from the training folds to the actual values from the test fold.

4. **Repeat steps 2 - 3**\
In the example, if you were using k = 5, then you would pick one of the folds you have not previously used and make it the test fold. The remaining  4 will become the training fold. In this way, every observation will be a member of the test fold once and training fold 4 times.

5. **Calculate the mean error**\
Measure the error (4) after each fold has been used as the test fold. Take the mean measure error of all folds from step 4.\
[@song2021making]

![k-Fold Cross-validation where k=5](CV5Fold_fig.png){#fig-kfold}


## Leave-one-out Cross-validations (LOOCV)

Leave-one-out Cross-validation is a specific case of k-Fold Cross-validation, where k is the number of observations in the dataset (5). 
With this method, the model is trained n times, where each training set has n−1 observations, and the test set has 1 observation [@fig-LOOCV].

$$
CV_{(n)} = \frac{1}{n}\sum_{i=1}^{n} \text{Measuer of Errori}_i \tag{5}
$$

**Process:**
The steps for LOOCV are almost identical to k-fold cross-validation. The only difference is that in k-fold, k must be less than the number of observations (n). In LOOCV, k=n, so when you split the data into testing and training data, the first testing fold is one of the observations and the training data would be every other observation [@fig-LOOCV]. In this way, every observation is tested against every other observation and the process would be repeated n times [@james2013introduction].

![Leave-one-out Cross-validation](LOOCV_fig.png){#fig-LOOCV}


## Nested Cross-validation

The process for Nested Cross-validation and k-fold CV methods start the same. 
The data are divided into k-folds, the model fit to k-1 and tested on the remaining k. 
In Nested CV this is called the *outer validation*. 
The difference with the Nested CV method is the addition of a *inner validation* step [@fig-NestCV], where a second k-fold CV is performed on the training set [@berrar2019cross].\

![Nested Cross-validation where k=5,2](NestCV_fig.png){#fig-NestCV}

# Analysis and Results

## Data Extraction, Transformation and Visualization

In our study, we analyzed a dataset from I-C Yeh's 1998 journal article [@yeh1998modeling] that models the compressive strength of high-performance concrete, using predictors such as Cement, Blast Furnace Slag, Fly Ash, Water, Superplasticizer, Coarse Aggregate, Fine Aggregate, and Age, with Strength as the response variable. Each predictor [@tbl-data], is measured in kilograms per cubic meter, except Age, which is measured in days [@misc_concrete_compressive_strength_165].
 
::: {#tbl-data}

| Name                | Data Type    | Units              | Variable  |
|:------------------- |:-------------|:-------------------|:----------|
| Strength            | Quantitative | MPa                | Response  |
| Cement              | Quantitative | kg in a m3 mixture | Predictor |
| Blast Furnace Slag  | Quantitative | kg in a m3 mixture | Predictor |
| Fly Ash             | Quantitative | kg in a m3 mixture | Predictor |
| Water               | Quantitative | kg in a m3 mixture | Predictor |
| Superplasticizer    | Quantitative | kg in a m3 mixture | Predictor |
| Coarse Aggregate    | Quantitative | kg in a m3 mixture | Predictor |
| Fine Aggregate      | Quantitative | kg in a m3 mixture | Predictor |
| Age                 | Quantitative | Day (1~365)        | Predictor |

: **Variables** {.striped .hover}

:::

The scatter plots reveal the relationships between these predictors and compressive strength, indicating that concrete strength generally increases with age as expected due to the curing process. Cement shows a strong positive correlation with strength, showing its importance as a binding agent. Superplasticizer also positively correlates with strength, suggesting that it enhances the mixture's workability and effectiveness. Water demonstrates a negative correlation, reflecting its potential to weaken the concrete matrix when used in excess. Aggregates and supplementary materials show smaller correlations, indicating indirect effects on strength. These visualizations gave us important insights into the dataset, which we used to guide our model selection and hyperparameter tuning during cross-validation.

### Load Libraries and Data
```{r, warning=FALSE, echo=TRUE}
#| label: load Libs and csv

# Load Libraries
library(dplyr)
library(readr)
library(knitr)
library(ggplot2)
library(stringr)
library(tidyr)
library(readxl)
library(caret)
library(mlbench)
library(corrplot)
library(Matrix)
library(lightgbm)
library(Metrics)
# library(patchwork)
# library(tictoc)

#Load Data
data <- read_csv("Concrete_Data.csv") %>%
  rename(Cement = 1,
         FurnacSlag = 2,
         FlyAsh = 3,
         Water = 4,
         Superplasticizer = 5,
         CoarseAggregate = 6,
         FineAggregate = 7,
         Age = 8,
         Strength = 9 ) %>%
  relocate(Strength)
```

### Data Plots
```{r, warning=FALSE, echo=TRUE}
#| label: plot_data

# Variable xy-point plots
pivot_longer(data = data, 2:9, names_to = "grp") %>% 
  mutate(grp = str_to_title(str_replace(grp, "_"," "))) %>%
  ggplot(aes(x=value, y = Strength)) +
  geom_point()+
  facet_wrap(~grp, scales = "free", ncol = 3)+
  xlab("Predictor")+ 
  ylab("Strength")+
  theme(strip.text = element_text(face = "bold"))+
  geom_smooth(formula = "y~x", method=lm,se = F, color="blue")
```

### Data Correlation Plots
```{r, warning=FALSE, echo=TRUE}
#| label: plot_corr

# Correlation plots
cor_matrix <- cor(data)

# Correlation circle plot
# corrplot(cor_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 45)

# Correlation Bar plot
response_correlations <- cor_matrix[, "Strength"]
response_correlations <- response_correlations[!names(response_correlations) %in% "Strength"]
sorted_correlations <- sort(response_correlations, decreasing = TRUE)

cor_df <- data.frame(Variable = names(sorted_correlations), Correlation = sorted_correlations)
ggplot(cor_df, aes(x = reorder(Variable, Correlation), y = Correlation)) +
  geom_bar(stat="identity", color = "blue4", fill = "gray78") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Correlation with Concrete Compressive Strength", 
       x = "Predictor Variable", 
       y = "Correlation")

```



# Analysis and Results


## Construct The Linear Regression Model



```{r, warning=FALSE, echo=TRUE}
#| label: analysis_model

# Subset data
predictors <- data[, c("Cement", "Superplasticizer", "Age", "Water")]
target <- data$Strength
data_combined <- data.frame(predictors, Strength = target)

# 
set.seed(123)
trainIndex <- createDataPartition(data_combined$Strength, p = .8, list = FALSE, times = 1)
train_data <- data_combined[trainIndex,]
test_data <- data_combined[-trainIndex,]

model <- train(Strength ~ Cement + Superplasticizer + Age + Water, 
               data = train_data, 
               method = "lm")
```

```{css, echo = FALSE}
.big-text {
  font-size: 23px !important
}
```

::: {.big-text}

$$
\hat{Strength} = 
`r round(coef(model$finalModel),2)[1]` \text{ + }
`r round(coef(model$finalModel),2)[2]`_\text{Cement}\text{ + }
`r round(coef(model$finalModel),2)[3]`_\text{Superplasticizer}\text{ + }
`r round(coef(model$finalModel),2)[4]`_\text{Age } 
`r round(coef(model$finalModel),2)[5]`_\text{Water}
$$

:::

### Linear Regression: k-Fold Cross-validation

```{r, warning=FALSE, echo=TRUE}
#| label: analysis_kfold

# Measure of error functions
calculate_rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
  }

calculate_mae <- function(actual, predicted) {
  mean(abs(actual - predicted))
  }

calculate_r2 <- function(actual, predicted) {
  1 - (sum((actual - predicted)^2) / sum((actual - mean(actual))^2))
  }

# k-Fold CV
set.seed(123)
train_control_kfold <- trainControl(method = "cv", number = 10)
model_kfold <- train(Strength ~ Cement + Superplasticizer + Age + Water, 
                     data = train_data, 
                     method = "lm", 
                     trControl = train_control_kfold)
kfold_predictions <- predict(model_kfold, newdata = test_data) 
kfold_rmse <- calculate_rmse(test_data$Strength, kfold_predictions) %>% round(2)
kfold_mae <- calculate_mae(test_data$Strength, kfold_predictions) %>% round(2)
kfold_r2 <- calculate_r2(test_data$Strength, kfold_predictions) %>% round(2)

Measure_of_Error <- c("RMSE","MAE", "R2" )
Result_Value <- c(kfold_rmse, kfold_mae, kfold_r2)
kfold_df <- data.frame(Measure_of_Error, Result_Value) 
kfold_df %>%  kable()

```

### Linear Regression: Leave-one-out Cross-validation

```{r, warning=FALSE, echo=TRUE}
#| label: analysis_loocv
set.seed(123)
train_control_loocv <- trainControl(method = "LOOCV")
model_loocv <- train(Strength ~ Cement + Superplasticizer + Age + Water, 
                     data = train_data, 
                     method = "lm", 
                     trControl = train_control_loocv)
loocv_predictions <- predict(model_loocv, newdata = test_data)
loocv_rmse <- calculate_rmse(test_data$Strength, loocv_predictions) %>% round(2)
loocv_mae <- calculate_mae(test_data$Strength, loocv_predictions) %>% round(2)
loocv_r2 <- calculate_r2(test_data$Strength, loocv_predictions) %>% round(2)

Result_Value <- c(loocv_rmse, loocv_mae, loocv_r2)
loocv_df <- data.frame(Measure_of_Error, Result_Value) 
loocv_df %>%  kable()
```

### Linear Regression Model: Nested Cross-validation

```{r, warning=FALSE, echo=TRUE}
#| label: analysis_nest

set.seed(123)
outer_train_control <- trainControl(method = "cv", number = 5)
inner_train_control <- trainControl(method = "cv", number = 5)

nested_model <- function(data, indices) {
  train_data <- data[indices,]
  test_data <- data[-indices,]
  
  model <- train(Strength ~ Cement + Superplasticizer + Age + Water, 
                 data = train_data, 
                 method = "lm", 
                 trControl = inner_train_control)
  predictions <- predict(model, newdata = test_data)
  rmse <- calculate_rmse(test_data$Strength, predictions)
  mae <- calculate_mae(test_data$Strength, predictions)
  r2 <- calculate_r2(test_data$Strength, predictions)
  return(c(rmse, mae, r2))
}

nested_cv_indices <- createFolds(data_combined$Strength, k = 5, list = TRUE, returnTrain = TRUE)
nested_cv_results <- t(sapply(nested_cv_indices, nested_model, data = data_combined))
nested_cv_rmse <- mean(nested_cv_results[, 1]) %>% round(2)
nested_cv_mae <- mean(nested_cv_results[, 2]) %>% round(2)
nested_cv_r2 <- mean(nested_cv_results[, 3]) %>% round(2)

Result_Value <- c(nested_cv_rmse, nested_cv_mae, nested_cv_r2)
nested_cv_df <- data.frame(Measure_of_Error, Result_Value) 
nested_cv_df %>%  kable()

```

Here we developed a linear regression model to predict concrete compressive strength using
predictors like Cement, Superplasticizer, Age, and Water. The resulting equation showed that
while Cement, Superplasticizer, and Age positively influenced concrete strength, higher Water
content had a negative impact. We assessed the model's performance using our three cross-
validation methods. The k-fold and LOOCV approaches gave similar results, with moderate
predictive accuracy and an R2 value indicating that about 46% of the variability in concrete
strength was explained by the model. These consistent results suggest reliability across different
data subsets, though LOOCV's computational cost is higher due to its multiple iterations. Nested
cross-validation provided slightly improved outcomes, enhancing the model's fit and
generalizability by isolating hyperparameter tuning from performance evaluation. Overall, this
analysis emphasizes the importance of choosing appropriate validation techniques to ensure
accurate model assessments. The study shows that while k-fold and LOOCV offer consistent
evaluations, nested cross-validation gives a slight edge in accuracy, underlining the significance
of cross-validation in developing reliable predictive models for concrete strength. This helps
facilitate more efficient and dependable construction practices.

## Construct The LightGBM Model



```{r, warning=FALSE, echo=TRUE}
#| label: analysis_lightgbm

# Here we are selecting the predictors and target variable. We combine them into a new dataframe. 
predictors <- data[, c("Cement", "Superplasticizer", "Age", "Water")]
target <- data$Strength
data_combined <- data.frame(predictors, Strength = target)

# Here we set a seed for reproducibility and create a partition index to split the data into 80/20. 
# We then split the dataframe into the training and test datasets based on the partition index. 
set.seed(123)
trainIndex <- createDataPartition(data_combined$Strength, p = .8, list = FALSE, times = 1)
train_data <- data_combined[trainIndex,]
test_data <- data_combined[-trainIndex,]

# Here we convert the training and testing data into matrix format for LightGBM.
# We also extract the target variable for both training and testing data. 
train_matrix <- as.matrix(train_data[, -ncol(train_data)])
train_label <- train_data$Strength
test_matrix <- as.matrix(test_data[, -ncol(test_data)])
test_label <- test_data$Strength

# Here we create a lightGBM dataset from the training data matrix and labels. 
dtrain <- lgb.Dataset(data = train_matrix, label = train_label)

# Here we define our paramaters for the LightGBM model.
params <- list(
  objective = "regression",
  metric = "rmse",
  learning_rate = 0.1,
  num_leaves = 31,
  max_depth = -1
)

# Here we train the lightGBM model with the specific paramaters and 100 rounds. 
model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1)
```


### LightGBM Model k-Fold Cross-validation
```{r, warning=FALSE, echo=TRUE}
#| label: analysis_lightgbm_kfold

#Redefining evaluation functions
calculate_rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

calculate_mae <- function(actual, predicted) {
  mean(abs(actual - predicted))
}

calculate_r2 <- function(actual, predicted) {
  ss_total <- sum((actual - mean(actual))^2)
  ss_residual <- sum((actual - predicted)^2)
  r2 <- 1 - (ss_residual / ss_total)
  return(r2)
}

#k-Fold CV for LightGBM
set.seed(123)
folds <- createFolds(train_data$Strength, k = 5, list = TRUE)
kfold_rmse_values <- c()
kfold_mae_values <- c()
kfold_r2_values <- c()

for (i in 1:5) {
  fold_train <- train_data[folds[[i]], ]
  fold_test <- train_data[-folds[[i]], ]

  dtrain <- lgb.Dataset(data = as.matrix(fold_train[, -ncol(fold_train)]), 
                        label = fold_train$Strength)
  dtest <- as.matrix(fold_test[, -ncol(fold_test)])

  params <- list(objective = "regression", metric = "rmse", learning_rate = 0.1, num_leaves = 31, max_depth = -1)
  model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1)

  predictions <- predict(model, dtest)

  kfold_rmse_values[i] <- calculate_rmse(fold_test$Strength, predictions)
  kfold_mae_values[i] <- calculate_mae(fold_test$Strength, predictions)
  kfold_r2_values[i] <- calculate_r2(fold_test$Strength, predictions)
}

kfold_rmse <- mean(kfold_rmse_values) %>% round(2)
kfold_mae <- mean(kfold_mae_values) %>% round(2)
kfold_r2 <- mean(kfold_r2_values) %>% round(2)

Result_Value <- c(kfold_rmse, kfold_mae, kfold_r2)
kfold_lgbm_df <- data.frame(Measure_of_Error, Result_Value)
kfold_lgbm_df %>%  kable()

```

### LightGBM Model: Leave-one-out Cross-validation

```{r, warning=FALSE, echo=TRUE}
#| label: analysis_lightgbm_loocv

set.seed(123)
train_control_loocv <- trainControl(method = "LOOCV")
loocv_predictions <- c()

for (i in 1:nrow(train_data)) {
  fold_train <- train_data[-i, ]
  fold_test <- train_data[i, , drop = FALSE]
  
  dtrain <- lgb.Dataset(data = as.matrix(fold_train[, -ncol(fold_train)]), 
                        label = fold_train$Strength)
  dtest <- as.matrix(fold_test[, -ncol(fold_test)])
  
  params <- list(objective = "regression", 
                 metric = "rmse", 
                 learning_rate = 0.1, 
                 num_leaves = 31, 
                 max_depth = -1)
  model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1)
  
  prediction <- predict(model, dtest)
  loocv_predictions <- c(loocv_predictions, prediction)
  }

loocv_rmse <- calculate_rmse(train_data$Strength, loocv_predictions) %>% round(2)
loocv_mae <- calculate_mae(train_data$Strength, loocv_predictions) %>% round(2)
loocv_r2 <- calculate_r2(train_data$Strength, loocv_predictions) %>% round(2)

Result_Value <- c(loocv_rmse, loocv_mae, loocv_r2)
loocv_lgbm_df <- data.frame(Measure_of_Error, Result_Value)
loocv_lgbm_df %>%  kable()
```

### LightGBM Model: Nested Cross-validation

```{r, warning=FALSE, echo=TRUE}
#| label: analysis_lightgbm_nest

# LightGBM: Nested CV
set.seed(123)
outer_folds <- createFolds(data_combined$Strength, k = 5, list = TRUE)
nested_cv_rmse_values <- c()
nested_cv_mae_values <- c()
nested_cv_r2_values <- c()

for (i in 1:5) {
  outer_train <- data_combined[outer_folds[[i]], ]
  outer_test <- data_combined[-outer_folds[[i]], ]
  
  inner_folds <- createFolds(outer_train$Strength, k = 2, list = TRUE)
  inner_cv_rmse_values <- c()
  
  for (j in 1:2) {
    inner_train <- outer_train[inner_folds[[j]], ]
    inner_test <- outer_train[-inner_folds[[j]], ]
    
    dtrain <- lgb.Dataset(data = as.matrix(inner_train[, -ncol(inner_train)]), 
                          label = inner_train$Strength)
    dtest <- as.matrix(inner_test[, -ncol(inner_test)])
    
    params <- list(objective = "regression", 
                   metric = "rmse", 
                   learning_rate = 0.1, 
                   num_leaves = 31, 
                   max_depth = -1)
    model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1)
    
    predictions <- predict(model, dtest)
    inner_cv_rmse_values[j] <- calculate_rmse(inner_test$Strength, predictions)
  }
  
  best_inner_rmse <- min(inner_cv_rmse_values)
  
  dtrain <- lgb.Dataset(data = as.matrix(outer_train[, -ncol(outer_train)]), label = outer_train$Strength)
  dtest <- as.matrix(outer_test[, -ncol(outer_test)])
  
  model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1)
  predictions <- predict(model, dtest)
  
  nested_cv_rmse_values[i] <- calculate_rmse(outer_test$Strength, predictions)
  nested_cv_mae_values[i] <- calculate_mae(outer_test$Strength, predictions)
  nested_cv_r2_values[i] <- calculate_r2(outer_test$Strength, predictions)
}

nested_cv_rmse <- mean(nested_cv_rmse_values) %>% round(2)
nested_cv_mae <- mean(nested_cv_mae_values) %>% round(2)
nested_cv_r2 <- mean(nested_cv_r2_values) %>% round(2)

Result_Value <- c(nested_cv_rmse, nested_cv_mae, nested_cv_r2)
nested_cv_lgbm_df <- data.frame(Measure_of_Error, Result_Value) 
nested_cv_lgbm_df %>%  kable()

```

In this analysis, we constructed a LightGBM model to predict concrete compressive strength and
evaluated its performance using k-fold, leave-one-out, and nested cross-validation methods.
LightGBM is a gradient boosting framework known for its efficiency and effectiveness with
structured datasets. Across all three cross-validation methods, the LightGBM model showed
strong predictive accuracy, consistently outperforming the linear regression model. The model
captured a significant amount of variability in the concrete strength data, showcasing its ability
to provide a better fit and explain more variance. Leave-one-out cross-validation, while
computationally intensive, provided the most accurate estimates, indicating an excellent model fit with a very low bias. Nested cross-validation, although showing a slight decrease in variance
explanation, offered a strong assessment by ensuring that hyperparameter tuning did not interfere
with model evaluation. Overall, the LightGBM model's performance across these methods shows
its suitability for complex datasets and its capability to predict concrete strength more accurately
than linear regression. This shows LightGBM's potential for strong application in real-world
scenarios where precise predictions are needed.

## Result Table and Plots

Comparing models is crucial for understanding their predictive performance, as emphasized by
[@hawkins2003assessing]. In our evaluation using three cross-validation techniques, the LightGBM model
consistently demonstrated superior performance over the linear regression model. Across the
board, LightGBM showed lower error rates and higher R2 values, indicating it provides a better
fit and explains more variability in the concrete strength data. In particular, LightGBM's
performance during LOOCV was notable for its accuracy and reliability, showing its capacity to
generalize well to new data. Nested cross-validation further confirmed LightGBM's effectiveness
by providing strong performance metrics that ensure the model's generalizability while
preventing overfitting. Overall, the LightGBM model's consistent excellence across all validation
methods shows its suitability for predicting concrete strength, demonstrating its potential as a
more efficient and accurate modeling approach compared to linear regression.

### Cross-validation Method Comparision
```{r, warning=FALSE, echo=TRUE}
#| label: analysis_results_mlr

# Adding method to CV data frames
kfold_results  <- kfold_df %>% mutate(Method = "5-Fold") 
LOOCV_results  <- loocv_df %>% mutate(Method = "LOOCV")
nested_results <- nested_cv_df %>% mutate(Method = "Nested CV") 

# Combining 3 CV data frames.
# Result Summary Table
result_long_df <- combine(kfold_results, LOOCV_results, nested_results)

# Pivot wider tidy up
result_wide_df <- result_long_df %>% 
  pivot_wider(names_from = Method, values_from = Result_Value) #%>% 
  # arrange(desc(R2), RMSE, MAE)
result_wide_df %>% kable()

# Summary Plot
summary_plot <- result_long_df %>% 
  ggplot(aes(x = Method, y = Result_Value)) +
  geom_bar(stat="identity", color = "blue4", fill = "gray78") +
  # facet_wrap(~ Measure_of_Error, scales = "free", ncol = 3) +
  facet_wrap(~factor(Measure_of_Error, c("RMSE","MAE","R2")), scales = "free", ncol = 3) +
  labs(title = "Linear Model",
       y = "Mean Error",
       x = "Cross-validation Method") + 
  theme(axis.title.x = element_text(face = "bold")) + 
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.title = element_text(face = "bold")) +
  theme(strip.text = element_text(face = "bold"))

summary_plot
```

### Model Comparison
```{r, warning=FALSE, echo=TRUE}
#| label: analysis_results_LGMB

# Model Comparison Dataframe
Comparison_df <- rbind(
  Regression_R <- cbind(result_long_df, c(replicate(length(result_long_df),"Linear_Regression"))) %>% rename(Model=4),
  LightGBM_KFold_R <- kfold_lgbm_df %>% mutate(Method = "5-Fold", Model = "LightGBM"),
  LightGBM_KFold_R <- loocv_lgbm_df %>% mutate(Method = "LOOCV", Model = "LightGBM"),
  LightGBM_Nest_R  <- nested_cv_lgbm_df %>% mutate(Method = "Nested CV", Model = "LightGBM")
  ) 

# print Comparison_df
Comparison_df %>% 
  pivot_wider(names_from = Model, values_from = Result_Value) %>% 
  relocate(Method) %>% 
  kable()

# Model Comparison Plot: k-Fold
Comparison_df %>%
  filter(Method == "5-Fold") %>% 
  ggplot(aes(x=Model, y=Result_Value)) +
  geom_bar(stat="identity", color = "blue4", fill = "gray78") +
  # facet_wrap(~ Measure_of_Error, scales = "free", ncol = 3) +
  facet_wrap(~factor(Measure_of_Error, c("RMSE","MAE","R2")), scales = "free", ncol = 3) +
  labs(title = "Model Comparison",
       subtitle = "k-Fold Crosss-validation where k=5",
       y = "Mean Error",
       x = "Model") + 
  theme(axis.title.x = element_text(face = "bold")) + 
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.title = element_text(face = "bold")) +
  theme(strip.text = element_text(face = "bold"))

# Model Comparison Plot: LOOCV 
Comparison_df %>% 
  filter(Method == "LOOCV") %>% 
  ggplot(aes(x=Model, y=Result_Value)) +
  geom_bar(stat="identity", color = "blue4", fill = "gray78") +
  # facet_wrap(~ Measure_of_Error, scales = "free", ncol = 3) +
  facet_wrap(~factor(Measure_of_Error, c("RMSE","MAE","R2")), scales = "free", ncol = 3) +
  labs(title = "Model Comparison",
       subtitle = "Leave-one-out Crosss-validation",
       y = "Mean Error",
       x = "Model") + 
  theme(axis.title.x = element_text(face = "bold")) + 
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.title = element_text(face = "bold")) +
  theme(strip.text = element_text(face = "bold"))

# Model Comparison Plot: Nested CV
Comparison_df %>%
  filter(Method == "Nested CV") %>% 
  ggplot(aes(x=Model, y=Result_Value)) +
  geom_bar(stat="identity", color = "blue4", fill = "gray78") +
  # facet_wrap(~ Measure_of_Error, scales = "free", ncol = 3) +
  facet_wrap(~factor(Measure_of_Error, c("RMSE","MAE","R2")), scales = "free", ncol = 3) +
  labs(title = "Model Comparison",
       subtitle = "Nested Crosss-validation",
       y = "Mean Error",
       x = "Model") + 
  theme(axis.title.x = element_text(face = "bold")) + 
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.title = element_text(face = "bold")) +
  theme(strip.text = element_text(face = "bold"))

```


# Conclusion

To conclude our study on predicting concrete strength using cross-validation techniques, we developed and evaluated two models: a linear regression model and a LightGBM model. Our analysis employed k-fold, leave-one-out cross-validation (LOOCV), and nested cross-validation methods to assess the robustness and accuracy of these models. The linear regression model served as a baseline, providing insights into the linear relationships between the concrete components and the compression strength. Meanwhile, the LightGBM model, known for its flexibility and performance with non-linear data, was used to capture more complex interactions within the dataset.\

The results of our cross-validation evaluations showed distinct patterns in model performance. The k-fold cross-validation method revealed that the LightGBM model consistently outperformed the linear regression model, demonstrating its capability to better generalize the data. LOOCV, while computationally intensive, provided similar insights, confirming the robustness of the LightGBM model's performance across individual data points. Nested cross-validation further validated these findings by mitigating overfitting, ensuring that our models were not just tailored to specific validation sets but had genuine predictive power.\

Cross-validation methods, as a whole, proved very important in providing a comprehensive evaluation of our models, specifically showing their strengths and weaknesses. These methods are key for future research and studies, because they offer a precise framework for assessing model performance and ensuring generalizability. The insights from this study show the importance of employing advanced cross-validation techniques in predictive modeling, allowing for more reliable and accurate applications in various different fields. By continuously refining these methods and exploring their implications, future research can further enhance model validation processes and contribute to the development of strong predictive models.

# References

All figures created by the authors.




