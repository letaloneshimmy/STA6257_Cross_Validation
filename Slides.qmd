---
title: "Evaluating Concrete Strength Model Performance"
subtitle: "Using Cross-validation Methods"
author: "Sai Devarashetty, Mattick, Musson, Perez"
date: '`r Sys.Date()`'
format:
  revealjs:
    scrollable: true
include-in-header:
  - text: |
      <style>
      #title-slide .title {
        font-size: 78px;
        color: #14316b;
      }
      #title-slide .subtitle {
        font-size: 55px;
        color: #14316b;
      }
      </style>
course: STA 6257 - Advanced Statistical Modeling
bibliography: [packages.bib, references.bib]
always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
echo: false
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
toc: false
theme: beige 
scrollable: true
slide-number: true
nocite: |
  @R-base, @R-caret, @R-Core, @R-corrplot, @R-dplyr, @R-ggplot2, @R-knitr, @R-lightgbm, @R-rmarkdown, @R-Matrix, @R-Metrics, @R-mlbench, @R-readr, @R-stringr, @R-tidyr
---



## Introduction To Cross-validation
```{css, echo = FALSE}
.tweak90-text {
  font-size: 89%;
}

.tweak80-text {
  font-size: 79%;
}

.big-text {
  font-size: 24px !important;
  font-weight: bold;
}
```

```{r, warning=FALSE, echo = FALSE}
#| label: load lib

# Load Libraries
library(dplyr)
library(readr)
library(knitr)
library(ggplot2)
library(stringr)
library(tidyr)
library(readxl)
library(caret)
library(mlbench)
library(corrplot)
library(Matrix)
library(lightgbm)
library(Metrics)
```

::: {.panel-tabset}

## Uses


  - **Measure** performance and generalizability of predictive models.
  - **Compare** different models constructed from the same data set. 

## Who Uses CV?

**CV widely used in various fields including:**

  - Machine Learning
  - Data Mining
  - Bioinformatics

## Goals

  - Minimize **overfitting**
  - Ensure a model **generalizes** to unseen data
  - Tune **hyperparameters**

:::

## Definitions

::: {.panel-tabset}

### Generalizability

**Generalizability**: \
How well predictive models created from a sample fit other samples from the same population.\


### Overfitting
:::{.tweak90-text}
**Overfitting**: \
When a model fits the the underlying patterns of the training data too well.\
\
Model fits characteristics specific to the training set:

  - <u>Noise</u>
  - <u>Random fluctuations</u>
  - <u>Outliers</u>
  
:::
### Hyperparameters

:::{.tweak90-text}
**Hyperparameters**:\
Are model configuration variables

:::: {.columns}

::: {.column width="50%"}
![Nodes and layers in a neural network](neural_network.png){width="75%"}
:::

::: {.column width="50%"}
![Branches in a decision tree](decision_tree.png){width="60%"}
:::
:::
::::
:::



## Process{.smaller}
::: {.panel-tabset}

### Subset
Subsets the data into k approximately equally sized folds

  - **Randomly**
  - **Without** replacement

![](process1.png)

[@song2021making]

### Split
Split The Subsets into **test** and **training sets**

  - **1** test set
  - **k-1** training set

![](process2.png)


  
### Train and Test

  - Fit the model to the **training set**
  - Apply the fitted model to the **test set**
  - Measure the prediction **error**

![](process3.png)


### Repeat
:::{.tweak90-text}
**Repeat k Times**

  - Train with **all** k-1 combinations
  - Test with each subset 1 time
![](process4.png){width=76%}
:::

### Mean Error

Calculate the **mean error**


![](process5.png)


:::


## Bias-Variance Trade-Off{.smaller}



:::{layout="[[-20,60,-20], [100], [100]]"}
:::{.tweak90-text}



| Method   | Computation | Bias         | Variance |
|:---------|:------------|:-------------|:---------|
| k-Fold   | Lower       | Intermediate | Lower    |
| LOOCV    | Highest     | Unbiased     | High     |
:<span style="color:blue">**k-Fold vs. LOOCV**</span>

__________

k-fold where k = 5 or k = 10 is recommended: \

  - Lowe **computational cost**\
  - Does not show excessive **bias**\
  - Does not show excessive **variance**

[@james2013introduction], [@gorriz2024k]
:::
:::

## Model Measures of Error (MOE){.smaller}
:::: {.panel-tabset}

## Overview



  - Measure the quality of fit of a model
  - Measuring error is a critical data modeling step
  - Different MOE for different data types
  
By measuring the quality of fit we can select the model that Generalizes best.




## MAE

**Mean Absolute Error**

$$
 \text{MAE} 
 = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{f}(x_i)|  
 \tag{1}
$$

  - A measure of error **magnitude**
  - The **sign** does not matter - *absolute value*
  - **Lower** magnitude indicates **better fit**
  - Take the mean absolute difference between:
    - observed $(y_i)$ and the predicted $\hat{f}(x_i)$ values
  - $\hat{f}(x_i)$ is the model prediction $\hat{f}$ for the ith observation
  - $n$ is observations




## RMSE

**Root Mean Squared Error**

$$
\text{RMSE} 
= \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2}
\tag{2}
$$
 
 - A measure of error **magnitude**
 - **Lower** magnitude indicates **better fit**
 - Error is **weighted**
   - **Squaring** the error give more weight to the larger ones
   - Taking the **square root** returns the error to the same units as the response variable




## $R^2$

**R-squared**

$$
\text{R}^2 
= \frac{SS_{tot}-SS_{res}}{SS_{tot}} 
= 1 - \frac{SS_{res}}{SS_{tot}}
= 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2}{\sum_{i=1}^{n}(y_i-\bar{f}(x_i))^2}
 \tag{3}
$$ 


  - Proportion of the **variance** explained by the predictor(s)
  - **Higher** value means better the fit
    - An $R^2$ value of 0.75 indicates 75% of the variance in the response variable is explained by the predictor(s)

[@james2013introduction], [@hawkins2003assessing], [@helsel1993statistical]

::::


## k-Fold Cross-validation
$$
CV_{(k)} = \frac{1}{k}\sum_{i=1}^{k} \text{Measuer of Errori}_i \tag{4}
$$
![](CV5Fold_fig.png#center){#fig-kfold}

[@james2013introduction], [@browne2000cross]


## Leave One Out Cross-validations (LOOCV)
$$
CV_{(n)} = \frac{1}{n}\sum_{i=1}^{n} \text{Measuer of Errori}_i \tag{5}
$$

![](LOOCV_fig.png){#fig-LOOCV}

[@james2013introduction], [@browne2000cross]


## Nested Cross-validation

![](NestCV_fig.png){#fig-NestCV}

[@berrar2019cross]


## Study Data 

 
Yeh modeled compression strength of high performance concrete (HPC) at various ages and made with different ratios of components [@yeh1998modeling].\
\
The data is available for downloaded at UCI Machine Learning Repository.\
[UCI Repository HPC Data](https://doi.org/10.24432/C5PK67)\
[@misc_concrete_compressive_strength_165]



## Data Exploration and Visualization{.smaller}

:::: {layout="[ 37, 63 ]"}

::: {#first-column}
  - **Target variable:**
    - Strength (MPa)
  - **Predictor variables:**
    - Cement (kg/m3)
    - Superplasticizer (kg/m3)
    - Age (days)
    - Water (kg/m3)

:::

::: {#second-column.#center}
```{r, warning=FALSE}
#| label: load csv

#Load Data
data <- read_csv("Concrete_Data.csv") %>%
  rename(Cement = 1,
         FurnacSlag = 2,
         FlyAsh = 3,
         Water = 4,
         Superplasticizer = 5,
         CoarseAggregate = 6,
         FineAggregate = 7,
         Age = 8,
         Strength = 9 ) %>%
  relocate(Strength)

# Correlation plots
cor_matrix <- cor(data)

# Correlation circle plot
# corrplot(cor_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 45)

# Correlation Bar plot
response_correlations <- cor_matrix[, "Strength"]
response_correlations <- response_correlations[!names(response_correlations) %in% "Strength"]
sorted_correlations <- sort(response_correlations, decreasing = TRUE)

cor_df <- data.frame(Variable = names(sorted_correlations), Correlation = sorted_correlations)
corr_plot <- ggplot(cor_df, aes(x = reorder(Variable, Correlation), y = Correlation)) +
  geom_bar(stat="identity", color = "blue4", fill = "gray78") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Correlation with Concrete Compressive Strength", x = "Predictor Variable", y = "Correlation")
corr_plot
```
:::{text-align="center"}
*All variables are quantitative*
:::
:::

::::

## Linear Regression Model{.smaller}
::: {#tbl-data}

```{r, warning=FALSE}
#| label: analysis_model

# Subset data
predictors <- data[, c("Cement", "Superplasticizer", "Age", "Water")]
target <- data$Strength
data_combined <- data.frame(predictors, Strength = target)

# 
set.seed(123)
trainIndex <- createDataPartition(data_combined$Strength, p = .8, list = FALSE, times = 1)
train_data <- data_combined[trainIndex,]
test_data <- data_combined[-trainIndex,]

model <- train(Strength ~ Cement + Superplasticizer + Age + Water, data = train_data, method = "lm")

model_summary <- summary(model$finalModel)[4]
model_summary %>% kable()
```

:::



::: {.big-text}

$$
\hat{Strength} = 
`r round(coef(model$finalModel),3)[1]` \text{ + }
`r round(coef(model$finalModel),3)[2]`_\text{Cement} +
`r round(coef(model$finalModel),3)[3]`_\text{Superplasticizer} +
`r round(coef(model$finalModel),3)[4]`_\text{Age }
`r round(coef(model$finalModel),3)[5]`_\text{Water}
$$

:::




## Linear Regression CV Results{.smaller}
::::{.tweak90-text}
::: {layout="[[30, 70], [100], [30,70], [100], [30,70]]"}

 - **k-Fold Results:**

```{r, warning=FALSE}
#| label: analysis_kfold

# Measure of error functions
calculate_rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
  }

calculate_mae <- function(actual, predicted) {
  mean(abs(actual - predicted))
  }

calculate_r2 <- function(actual, predicted) {
  1 - (sum((actual - predicted)^2) / sum((actual - mean(actual))^2))
  }

# k-Fold CV
set.seed(123)
train_control_kfold <- trainControl(method = "cv", number = 10)
model_kfold <- train(Strength ~ Cement + Superplasticizer + Age + Water, data = train_data, method = "lm", trControl = train_control_kfold)
kfold_predictions <- predict(model_kfold, newdata = test_data) 
kfold_rmse <- calculate_rmse(test_data$Strength, kfold_predictions) %>% round(2)
kfold_mae <- calculate_mae(test_data$Strength, kfold_predictions) %>% round(2)
kfold_r2 <- calculate_r2(test_data$Strength, kfold_predictions) %>% round(2)

Measure_of_Error <- c("RMSE","MAE", "R2" )
Result_Value <- c(kfold_rmse, kfold_mae, kfold_r2)
kfold_df <- data.frame(Measure_of_Error, Result_Value) 
kfold_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()
```



_______________________

   - **LOOCV Results:**

```{r, warning=FALSE}
#| label: analysis_loocv
set.seed(123)
train_control_loocv <- trainControl(method = "LOOCV")
model_loocv <- train(Strength ~ Cement + Superplasticizer + Age + Water, data = train_data, method = "lm", trControl = train_control_loocv)
loocv_predictions <- predict(model_loocv, newdata = test_data)
loocv_rmse <- calculate_rmse(test_data$Strength, loocv_predictions) %>% round(2)
loocv_mae <- calculate_mae(test_data$Strength, loocv_predictions) %>% round(2)
loocv_r2 <- calculate_r2(test_data$Strength, loocv_predictions) %>% round(2)

Result_Value <- c(loocv_rmse, loocv_mae, loocv_r2)
loocv_df <- data.frame(Measure_of_Error, Result_Value) 
loocv_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()
```


__________________________________________________


  - **Nested CV Results:**
  

```{r, warning=FALSE}
#| label: analysis_nest

set.seed(123)
outer_train_control <- trainControl(method = "cv", number = 5)
inner_train_control <- trainControl(method = "cv", number = 5)

nested_model <- function(data, indices) {
  train_data <- data[indices,]
  test_data <- data[-indices,]
  
  model <- train(Strength ~ Cement + Superplasticizer + Age + Water, data = train_data, method = "lm", trControl = inner_train_control)
  predictions <- predict(model, newdata = test_data)
  rmse <- calculate_rmse(test_data$Strength, predictions)
  mae <- calculate_mae(test_data$Strength, predictions)
  r2 <- calculate_r2(test_data$Strength, predictions)
  return(c(rmse, mae, r2))
}

nested_cv_indices <- createFolds(data_combined$Strength, k = 5, list = TRUE, returnTrain = TRUE)
nested_cv_results <- t(sapply(nested_cv_indices, nested_model, data = data_combined))
nested_cv_rmse <- mean(nested_cv_results[, 1]) %>% round(2)
nested_cv_mae <- mean(nested_cv_results[, 2]) %>% round(2)
nested_cv_r2 <- mean(nested_cv_results[, 3]) %>% round(2)

Result_Value <- c(nested_cv_rmse, nested_cv_mae, nested_cv_r2)
nested_cv_df <- data.frame(Measure_of_Error, Result_Value) 
nested_cv_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()

```

:::
::::

## LightGBM Model
:::: {layout="[[-20,60,-20], [100], [100]]"}
:::{.tweak90-text}

```{r, warning=FALSE}
#| label: analysis_lightgbm

#Here we are selecting the predictors and target variable. We combine them into a new dataframe. 
predictors <- data[, c("Cement", "Superplasticizer", "Age", "Water")]
target <- data$Strength
data_combined <- data.frame(predictors, Strength = target)

#Here we set a seed for reproducibility and create a partition index to split the data into 80/20. 
#We then split the dataframe into the training and test datasets based on the partition index. 
set.seed(123)
trainIndex <- createDataPartition(data_combined$Strength, p = .8, list = FALSE, times = 1)
train_data <- data_combined[trainIndex,]
test_data <- data_combined[-trainIndex,]

#Here we convert the training and testing data into matrix format for LightGBM.
#We also extract the target variable for both training and testing data. 
train_matrix <- as.matrix(train_data[, -ncol(train_data)])
train_label <- train_data$Strength
test_matrix <- as.matrix(test_data[, -ncol(test_data)])
test_label <- test_data$Strength

#Here we create a lightGBM dataset from the training data matrix and labels. 
dtrain <- lgb.Dataset(data = train_matrix, label = train_label)

#Here we define our paramaters for the LightGBM model.
params <- list(
  objective = "regression",
  metric = "rmse",
  learning_rate = 0.1,
  num_leaves = 31,
  max_depth = -1
)

#Here we train the lightGBM model with the specific paramaters and 100 rounds. 
model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1)


#Redefining evaluation functions
calculate_rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

calculate_mae <- function(actual, predicted) {
  mean(abs(actual - predicted))
}

calculate_r2 <- function(actual, predicted) {
  ss_total <- sum((actual - mean(actual))^2)
  ss_residual <- sum((actual - predicted)^2)
  r2 <- 1 - (ss_residual / ss_total)
  return(r2)
}

#k-Fold CV for LightGBM
set.seed(123)
folds <- createFolds(train_data$Strength, k = 5, list = TRUE)
kfold_rmse_values <- c()
kfold_mae_values <- c()
kfold_r2_values <- c()

for (i in 1:5) {
  fold_train <- train_data[folds[[i]], ]
  fold_test <- train_data[-folds[[i]], ]

  dtrain <- lgb.Dataset(data = as.matrix(fold_train[, -ncol(fold_train)]), label = fold_train$Strength)
  dtest <- as.matrix(fold_test[, -ncol(fold_test)])

  params <- list(objective = "regression", metric = "rmse", learning_rate = 0.1, num_leaves = 31, max_depth = -1)
  model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1)

  predictions <- predict(model, dtest)

  kfold_rmse_values[i] <- calculate_rmse(fold_test$Strength, predictions)
  kfold_mae_values[i] <- calculate_mae(fold_test$Strength, predictions)
  kfold_r2_values[i] <- calculate_r2(fold_test$Strength, predictions)
}

kfold_rmse <- mean(kfold_rmse_values) %>% round(2)
kfold_mae <- mean(kfold_mae_values) %>% round(2)
kfold_r2 <- mean(kfold_r2_values) %>% round(2)

Result_Value <- c(kfold_rmse, kfold_mae, kfold_r2)
kfold_lgbm_df <- data.frame(Measure_of_Error, Result_Value)
kfold_lgbm_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()
```


:::
_______________

:::{.tweak90-text}

  - Ensemble of decision trees
  - Uses gradient boosting
  - Final prediction is the sum of predictions from all individual trees
  - Feature importance

:::
::::



## LightGBM CV Results{.smaller}
::::{.tweak90-text}
::: {layout="[[30,70], [100], [30,70], [100], [30,70]]"}

  - **k-Fold Results:**

```{r, warning=FALSE}

kfold_lgbm_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()
```

________


  - **LOOCV Results:**

```{r, warning=FALSE}
#| label: analysis_lightgbm_loocv

set.seed(123)
train_control_loocv <- trainControl(method = "LOOCV")
loocv_predictions <- c()

for (i in 1:nrow(train_data)) {
  fold_train <- train_data[-i, ]
  fold_test <- train_data[i, , drop = FALSE]

  dtrain <- lgb.Dataset(data = as.matrix(fold_train[, -ncol(fold_train)]), label = fold_train$Strength)
  dtest <- as.matrix(fold_test[, -ncol(fold_test)])

  params <- list(objective = "regression", metric = "rmse", learning_rate = 0.1, num_leaves = 31, max_depth = -1)
  model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1) ############################################ Change nrounds = 100

  prediction <- predict(model, dtest)
  loocv_predictions <- c(loocv_predictions, prediction)
}

loocv_rmse <- calculate_rmse(train_data$Strength, loocv_predictions) %>% round(2)
loocv_mae <- calculate_mae(train_data$Strength, loocv_predictions) %>% round(2)
loocv_r2 <- calculate_r2(train_data$Strength, loocv_predictions) %>% round(2)

Result_Value <- c(loocv_rmse, loocv_mae, loocv_r2)
loocv_lgbm_df <- data.frame(Measure_of_Error, Result_Value)
loocv_lgbm_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()
```

________


  - **Nested CV Results:**

```{r, warning=FALSE}
#| label: analysis_lightgbm_nest

# LightGBM: Nested CV
set.seed(123)
outer_folds <- createFolds(data_combined$Strength, k = 5, list = TRUE)
nested_cv_rmse_values <- c()
nested_cv_mae_values <- c()
nested_cv_r2_values <- c()

for (i in 1:5) {
  outer_train <- data_combined[outer_folds[[i]], ]
  outer_test <- data_combined[-outer_folds[[i]], ]
  
  inner_folds <- createFolds(outer_train$Strength, k = 2, list = TRUE)
  inner_cv_rmse_values <- c()
  
  for (j in 1:2) {
    inner_train <- outer_train[inner_folds[[j]], ]
    inner_test <- outer_train[-inner_folds[[j]], ]
    
    dtrain <- lgb.Dataset(data = as.matrix(inner_train[, -ncol(inner_train)]), label = inner_train$Strength)
    dtest <- as.matrix(inner_test[, -ncol(inner_test)])
    
    params <- list(objective = "regression", metric = "rmse", learning_rate = 0.1, num_leaves = 31, max_depth = -1)
    model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1)
    
    predictions <- predict(model, dtest)
    inner_cv_rmse_values[j] <- calculate_rmse(inner_test$Strength, predictions)
  }
  
  best_inner_rmse <- min(inner_cv_rmse_values)
  
  dtrain <- lgb.Dataset(data = as.matrix(outer_train[, -ncol(outer_train)]), label = outer_train$Strength)
  dtest <- as.matrix(outer_test[, -ncol(outer_test)])
  
  model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1)
  predictions <- predict(model, dtest)
  
  nested_cv_rmse_values[i] <- calculate_rmse(outer_test$Strength, predictions)
  nested_cv_mae_values[i] <- calculate_mae(outer_test$Strength, predictions)
  nested_cv_r2_values[i] <- calculate_r2(outer_test$Strength, predictions)
}

nested_cv_rmse <- mean(nested_cv_rmse_values) %>% round(2)
nested_cv_mae <- mean(nested_cv_mae_values) %>% round(2)
nested_cv_r2 <- mean(nested_cv_r2_values) %>% round(2)

Result_Value <- c(nested_cv_rmse, nested_cv_mae, nested_cv_r2)
nested_cv_lgbm_df <- data.frame(Measure_of_Error, Result_Value) 
nested_cv_lgbm_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()
```

:::
::::

## Comparison of Models{.smaller}
:::: {layout="[35, 65]"}

::: {#first-column}
  - **Performance Comparison:**\
  \  Linear Regression vs. LightGBM\
  \
  \
  - Advantages and disadvantages\
  \ of each model

:::

::: {#second-column}

```{r, warning=FALSE}
#| label: analysis_results_mlr

# Adding method to CV data frames
kfold_results  <- kfold_df %>% mutate(Method = "5-Fold") 
LOOCV_results  <- loocv_df %>% mutate(Method = "LOOCV")
nested_results <- nested_cv_df %>% mutate(Method = "Nested CV") 

# Combining 3 CV data frames.
# Result Summary Table
result_long_df <- combine(kfold_results, LOOCV_results, nested_results)

# Pivot wider tidy up
result_wide_df <- result_long_df %>% 
  pivot_wider(names_from = Measure_of_Error, values_from = Result_Value) %>% 
  arrange(desc(R2), RMSE, MAE)
# result_wide_df %>% kable()
```


```{r, warning=FALSE}
#| label: analysis_results_LGMB

# Model Comparison Dataframe
Comparison_df <- rbind(
  Regression_R <- cbind(result_long_df, c(replicate(length(result_long_df),"Linear_Regression"))) %>% rename(Model=4),
  LightGBM_KFold_R <- kfold_lgbm_df %>% mutate(Method = "5-Fold", Model = "LightGBM"),
  LightGBM_KFold_R <- loocv_lgbm_df %>% mutate(Method = "LOOCV", Model = "LightGBM"),
  LightGBM_Nest_R  <- nested_cv_lgbm_df %>% mutate(Method = "Nested CV", Model = "LightGBM")
  ) 
  

# print Comparison_df
Comparison_df %>% 
  pivot_wider(names_from = Model, 
              values_from = Result_Value) %>% 
  relocate(Method) %>% 
  rename(`Measure of Error` = Measure_of_Error,
         `Linear Regression` = Linear_Regression) %>% 
  mutate(Method = if_else(Method == "Nested CV", "NCV", Method)) %>% 
  kable()
```
:::
::::



## Model Comparison k-Fold Plot

```{r, warning=FALSE}
# Model Comparison Plot: k-Fold
KFold_Plot <- Comparison_df %>%
  filter(Method == "5-Fold") %>%
  ggplot(aes(x=Model, y=Result_Value)) +
  geom_bar(stat="identity", color = "blue4", fill = "gray78") +
  # facet_wrap(~ Measure_of_Error, scales = "free", ncol = 3) +
  facet_wrap(~factor(Measure_of_Error, c("RMSE","MAE","R2")), scales = "free", ncol = 3) +
  labs(title = "Model Comparison",
       subtitle = "k-Fold Crosss-validation where k = 5",
       y = "Mean Error",
       x = "Model") +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.title = element_text(face = "bold")) +
  theme(strip.text = element_text(face = "bold"))
KFold_Plot
```

## Model Comparison LOOCV Plot
```{r, warning=FALSE}
# 
# print('Intentionally blank. Takes 10 minutes to render this code chunk \n Will turn it back on to finalize')

# Model Comparison Plot: LOOCV
LOOCV_Plot <- Comparison_df %>%
  filter(Method == "LOOCV") %>%
  ggplot(aes(x=Model, y=Result_Value)) +
  geom_bar(stat="identity", color = "blue4", fill = "gray78") +
  # facet_wrap(~ Measure_of_Error, scales = "free", ncol = 3) +
  facet_wrap(~factor(Measure_of_Error, c("RMSE","MAE","R2")), scales = "free", ncol = 3) +
  labs(title = "Model Comparison",
       subtitle = "Leave-one-out Cross-validation",
       y = "Mean Error",
       x = "Model") +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.title = element_text(face = "bold")) +
  theme(strip.text = element_text(face = "bold"))
LOOCV_Plot

```

## Model Comparison Nested CV Plot
```{r, warning=FALSE}
# Model Comparison Plot: Nested CV
Nested_CV_Plot <- Comparison_df %>%
  filter(Method == "Nested CV") %>%
  ggplot(aes(x=Model, y=Result_Value)) +
  geom_bar(stat="identity", color = "blue4", fill = "gray78") +
  # facet_wrap(~ Measure_of_Error, scales = "free", ncol = 3) +
  facet_wrap(~factor(Measure_of_Error, c("RMSE","MAE","R2")), scales = "free", ncol = 3) +
  labs(title = "Model Comparison",
       subtitle = "Nested Crosss-validation",
       y = "Mean Error",
       x = "Model") +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.title = element_text(face = "bold")) +
  theme(strip.text = element_text(face = "bold"))
Nested_CV_Plot
```



## Conclusion: Overview{.smaller}

  - **Evaluation of Two Models:**
    - Linear Regression Model
    - LightGBM Model\
    \
  - **Cross-validation Methods Used:**
    - k-fold Cross-validation
    - Leave-one-out Cross-validation (LOOCV)
    - Nested Cross-validation

## Conclusion: Key Findings{.smaller}

  - **Model Performance:**
    - LightGBM consistently outperformed Linear Regression
    - Linear Regression provided baseline insights into linear relationships\
    \
  - **Cross-validation Insights:**
    - k-fold CV showed LightGBM's superior generalization
    - LOOCV confirmed robustness across individual data points
    - Nested CV mitigated overfitting, ensuring genuine predictive power

## Conclusion: Implications and Future Directions{.smaller}

- **Implications for Future Research:**
  - Importance of advanced cross-validation techniques
  - Enhancing model validation processes
  - Ensuring model generalizability and reliability across various applications\
  \
- **Future Directions:**
  - Continuous refinement of cross-validation methods
  - Exploration of implications in different predictive modeling scenarios
  - Development of robust predictive models through improved validation processes

## References

All figures were created by the authors