---
title: "Evaluating Concrete Strength Model Performance"
subtitle: "Using Cross-validation Methods"
author: "Sai Devarashetty, Mattick, Musson, Perez"
date: '`r Sys.Date()`'
format:
  revealjs:
    # smaller: true
    scrollable: true
include-in-header:
  - text: |
      <style>
      #title-slide .title {
        font-size: 78px;
        color: #14316b;
      }
      </style>
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
echo: false
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
toc: false
theme: beige 
scrollable: true
slide-number: true
# toc-location: right
# toc-expand: true
# toc-depth: 2
# number-sections: true
---



## Introduction To Crossvalidation
```{css, echo = FALSE}
.tweak90-text {
  font-size: 89%;
}

.big-text {
  font-size: 27px !important;
  font-weight: bold;
}
```

::: {.panel-tabset}

## Uses


  - **Measure** performance and generalizability of machine learning and predictive models.
  - **Compare** different models constructed from the same data set. 

## Who Uses CV?

**CV widely used in various fields including:**

  - Machine Learning
  - Data Mining
  - Bioinformatics

## Goals

  - Minimize **overfitting**
  - Ensure a model **generalizes** to unseen data
  - Tune **hyperparameters**

:::

## Definitions

::: {.panel-tabset}

### Generalizability

**Generalizability**: \
How well predictive models created from a sample fit other samples from the same population.\


### Overfitting
:::{.tweak90-text}
**Overfitting**: \
When a model fits the the underlying patterns of the training data too well.\
\
Model fits characteristics specific to the training set:

  - <u>Noise</u>
  - <u>Random fluctuations</u>
  - <u>Outliers</u>
  
:::
### Hyperparameters

:::{.tweak90-text}
**Hyperparameters**:\
Are model configuration variables

:::: {.columns}

::: {.column width="50%"}
![Nodes and layers in a neural network](neural_network.png){width="75%"}
:::

::: {.column width="50%"}
![Branches in a decision tree](decision_tree.png){width="60%"}
:::
:::
::::
:::



## Process{.smaller}
::: {.panel-tabset}

### Subset
Subsets the data into K approximately equally sized folds

  - **Randomly**
  - **Without** replacement

![](process1.png)

[@song2021making]

### Split
Split The Subsets into **test** and **training sets**

  - **1** test set
  - **K-1** training set

![](process2.png)


  
### Train and Test

  - **Fit** the model to the training data
  - Apply the fitted model to the **test** set
  - Measure the prediction **error**

![](process3.png)


### Repeat
:::{.tweak90-text}
**Repeat K Times**

  - Fit to all K-1 combinations
  - Test with each subset 1 time
![](process4.png){width=76%}
:::

### Mean Error

Calculate the **mean error**


![](process5.png)


:::


## Bias-Variance Trade-Off{.smaller}



:::{layout="[[-20,60,-20], [100], [100]]"}
:::{.tweak90-text}



| Method   | Computation | Bias         | Variance |
|:---------|:------------|:-------------|:---------|
| K-Fold   | Lower       | Intermediate | Lower    |
| LOOCV    | Highest     | Unbiased     | High     |
:<span style="color:blue">**K-Fold vs. LOOCV**</span>

__________

K-fold where K = 5 or K = 10 is recommended: \

  - Lowe **computational cost**\
  - Does not show excessive **bias**\
  - Does not show excessive **variance**

[@james2013introduction], [@gorriz2024k]
:::
:::

## Model Measures of Error (MOE){.smaller}
::: {.panel-tabset}
## Overview{.smaller}

  - Measure the quality of fit of a model
  - Measuring error is a critical data modeling step
  - Different MOE for different data types
  
By measuring the quality of fit we can select the model that Generalizes best.

## Mean Absolute Error (MAE){.smaller}
:::{.tweak90-text}
$$
 \text{MAE} 
 = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{f}(x_i)|  
 \tag{1}
$$

  - A measure of error **magnitude**
  - The **sine** does not matter - *absolute value*
  - **lower** magnitude indicates **better fit**
  - Take the mean absolute difference between:
    - observed $(y_i)$ and the predicted $\hat{f}(x_i)$ values
  - $n$ is the number of observations,
  - $\hat{f}(x_i)$ is the model prediction $\hat{f}$ for the ith observation
  - $y_i$ is the observed value

:::

## Root Mean Squared Error (RMSE){.smaller}
$$
\text{RMSE} 
= \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2}
\tag{2}
$$
 
 - A measure of error **magnitude**
 - **lower** magnitude indicates **better fit**
 - Error is **weighted**
   - **Squaring** the error give more weight to the larger ones
   - Taking the **square root** returns the error to the same units as the response variable



## R-squared ($R^2$){.smaller}
$$
\text{R}^2 
= \frac{SS_{tot}-SS_{res}}{SS_{tot}} 
= 1 - \frac{SS_{res}}{SS_{tot}}
= 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2}{\sum_{i=1}^{n}(y_i-\bar{f}(x_i))^2}
 \tag{3}
$$ 


  - Proportion of the **variance** explained by the predictor(s)
  - **higher** value means better the fit
    - An $R^2$ value of 0.75 indicates 75% of the variance in the response variable is explained by the predictor(s)

[@james2013introduction], [@hawkins2003assessing], [@helsel1993statistical]
:::

## K-Fold Cross-Validation
$$
CV_{(k)} = \frac{1}{k}\sum_{i=1}^{k} \text{Measuer of Errori}_i \tag{4}
$$
![](CV5Fold_fig.png#center){#fig-kfold}

[@james2013introduction],[@browne2000cross]

<!-- **Process:** -->

<!-- 1. **Prepare the data**\ -->
<!-- Subsets the data randomly and without replacement into K equally sized folds.  -->
<!-- Each fold will contain approximately n/k observations.  -->
<!-- For example when n = 200 and K = 5, then each fold have 200/5 = 40 observations. If n = 201, then one of the folds would have 41, the other four folds would have 40 observations. -->

<!-- 2. **Split the folds into test and training sets**\ -->
<!-- In the previous example, if you had 5 folds, we could choose the first set to be the test set and the other 4 would be the training set. It doesnâ€™t make a difference which one you choose as all of the folds will eventually be test folds against the other 4 [@fig-kfold]. -->

<!-- 3. **Fit model to the training data**\ -->
<!-- Take the model you are going to use for prediction and fit it to the training data. Continuing with our example, you would use the 4 training folds to fit the model. Take the fitted model you developed in step 3 and apply it to the 1 test fold. After applying it to the model, you would take the resulting prediction and determine the accuracy by comparing what was predicted from the training folds to the actual values from the test fold. -->

<!-- 4. **Repeat steps 2 - 4**\ -->
<!-- In the example, if you were using K = 5, then you would pick one of the folds you have not previously used and make it the test fold and the other 4 the training fold. In this way, every observation will be a member of the test fold once and training folds 4 times. -->

<!-- 5. **Calculate the mean error**\ -->
<!-- Measure the error after each fold has been used as the test fold. Take the mean measure error of all folds from step 4.\ -->
<!-- [@song2021making] -->




## Leave One Out Cross-validations (LOOCV)
$$
CV_{(n)} = \frac{1}{n}\sum_{i=1}^{n} \text{Measuer of Errori}_i \tag{5}
$$

![](LOOCV_fig.png){#fig-LOOCV}

[@james2013introduction],[@browne2000cross]

<!-- **Process:** -->
<!-- The steps for LOOCV are almost identical to k-fold cross-validation. The only difference is that in K-fold, K must be less than the number of observations (n). In LOOCV, K = n, so when you split the data into testing and training data, the first testing fold is one of the observations and the training data would be every other observation [@fig-LOOCV]. In this way, every observation is tested against every other observation and the process would be repeated n times [@james2013introduction]. -->



## Nested Cross-Validation

![](NestCV_fig.png){#fig-NestCV}

[@berrar2019cross]
<!-- 1. **Split the data into training and testing sets**\ -->
<!-- As in k-fold cross-validation, break the observations into the single test fold and the training folds.  For example, if there are 300 observations and you use K = 5, four of the folds would be training folds and one of them would be the training fold. -->

<!-- 2. **Define inner and outer loops**\ -->
<!-- We define the test fold as the outer loop and use that to test the performance of the model.  The training loops will be defined as the inner loop and we will use that to test which parameters we should use. -->

<!-- 3. **Split the inner loop into training sets and validation sets**\ -->
<!-- The inner loop (or training folds) is broken in half.  Half of that data will be used as training and the other half will be used as evaluation [@fig-NestCV]. -->

<!-- 4. **Fit the model to the inner loop**\ -->
<!-- We choose the number of parameters that we are going to use for validation and fit it to the model.  After fitting, you will store the accuracy value for those parameters.  We then switch the validation and training sets from the inner loop and fit them to the model.  After receiving another accuracy score, we would average them together with the previous accuracy score for that number of parameters. -->

<!-- 5. **Choose another number of parameters**\ -->
<!-- We would then choose a different number of parameters and repeat step 4.  After determining the average accuracy for the new set of parameters, we would compare it to the average accuracy produced by the other parameters.  The number of parameters that produces the highest average accuracy is chosen for that training fold. -->

<!-- 6. **Repeat the process K-times**\ -->
<!-- After getting an accuracy score for each training fold, we find the average of all folds which will give us the average accuracy of the model.\ [@berrar2019cross]. -->




## Study Data 

 
[@yeh1998modeling] modeled compression strength of high performance concrete (HPC) at various ages and made with different ratios of components.
The data used for their study was made publicly available and can be downloaded UCI Machine Learning Repository [@misc_concrete_compressive_strength_165]. 



## Data Exploration and Visulation{.smaller}

:::: {layout="[ 35, 65 ]"}

::: {#first-column}
  - Target variable:
    - Strength in MPa
  - Predictor variables:
    - Cement in kg in a m3 mixture
    - Superplasticizer kg in a m3 mixture
    - Age in days
    - Water kg in a m3 mixture

:::

::: {#second-column.#center}
```{r, warning=FALSE}
#| label: load Libs and csv

# Load Libraries
library(dplyr)
library(readr)
library(knitr)
library(ggplot2)
library(stringr)
library(tidyr)
library(readxl)
library(caret)
library(mlbench)
library(corrplot)
library(Matrix)
library(lightgbm)
library(Metrics)

#Load Data
data <- read_csv("Concrete_Data.csv") %>%
  rename(Cement = 1,
         FurnacSlag = 2,
         FlyAsh = 3,
         Water = 4,
         Superplasticizer = 5,
         CoarseAggregate = 6,
         FineAggregate = 7,
         Age = 8,
         Strength = 9 ) %>%
  relocate(Strength)

# Correlation plots
cor_matrix <- cor(data)

# Correlation circle plot
# corrplot(cor_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 45)

# Correlation Bar plot
response_correlations <- cor_matrix[, "Strength"]
response_correlations <- response_correlations[!names(response_correlations) %in% "Strength"]
sorted_correlations <- sort(response_correlations, decreasing = TRUE)

cor_df <- data.frame(Variable = names(sorted_correlations), Correlation = sorted_correlations)
corr_plot <- ggplot(cor_df, aes(x = reorder(Variable, Correlation), y = Correlation)) +
  geom_bar(stat="identity", color = "blue4", fill = "gray78") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Correlation with Concrete Compressive Strength", x = "Predictor Variable", y = "Correlation")
corr_plot
```
:::{text-align="center"}
*All variables are quantitative*
:::
:::

::::

## Linear Regression Model{.smaller}
::: {#tbl-data}

```{r, warning=FALSE}
#| label: analysis_model

# Subset data
predictors <- data[, c("Cement", "Superplasticizer", "Age", "Water")]
target <- data$Strength
data_combined <- data.frame(predictors, Strength = target)

# 
set.seed(123)
trainIndex <- createDataPartition(data_combined$Strength, p = .8, list = FALSE, times = 1)
train_data <- data_combined[trainIndex,]
test_data <- data_combined[-trainIndex,]

model <- train(Strength ~ Cement + Superplasticizer + Age + Water, data = train_data, method = "lm")

model_summary <- summary(model$finalModel)[4]
model_summary %>% kable()
```

:::



::: {.big-text}

$$
\hat{Strength} = 
`r round(coef(model$finalModel),3)[1]`_\text{Cement + }
`r round(coef(model$finalModel),3)[2]`_\text{Superplasticizer + } 
`r round(coef(model$finalModel),3)[3]`_\text{Age } 
`r round(coef(model$finalModel),3)[4]`_\text{Water}
$$

:::




## Linear Regression CV Results{.smaller}
::::{.tweak90-text}
::: {layout="[[30,70], [100], [30,70], [100], [30,70]]"}

 - **K-Fold Results:**

```{r, warning=FALSE}
#| label: analysis_kfold

# Measure of error functions
calculate_rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
  }

calculate_mae <- function(actual, predicted) {
  mean(abs(actual - predicted))
  }

calculate_r2 <- function(actual, predicted) {
  1 - (sum((actual - predicted)^2) / sum((actual - mean(actual))^2))
  }

# K-Fold CV
set.seed(123)
train_control_kfold <- trainControl(method = "cv", number = 10)
model_kfold <- train(Strength ~ Cement + Superplasticizer + Age + Water, data = train_data, method = "lm", trControl = train_control_kfold)
kfold_predictions <- predict(model_kfold, newdata = test_data) 
kfold_rmse <- calculate_rmse(test_data$Strength, kfold_predictions) %>% round(2)
kfold_mae <- calculate_mae(test_data$Strength, kfold_predictions) %>% round(2)
kfold_r2 <- calculate_r2(test_data$Strength, kfold_predictions) %>% round(2)

Measure_of_Error <- c("RMSE","MAE", "R2" )
Result_Value <- c(kfold_rmse, kfold_mae, kfold_r2)
kfold_df <- data.frame(Measure_of_Error, Result_Value) 
kfold_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()

```

_______________________

   - **LOOCV Results:**

```{r, warning=FALSE}
#| label: analysis_loocv
set.seed(123)
train_control_loocv <- trainControl(method = "LOOCV")
model_loocv <- train(Strength ~ Cement + Superplasticizer + Age + Water, data = train_data, method = "lm", trControl = train_control_loocv)
loocv_predictions <- predict(model_loocv, newdata = test_data)
loocv_rmse <- calculate_rmse(test_data$Strength, loocv_predictions) %>% round(2)
loocv_mae <- calculate_mae(test_data$Strength, loocv_predictions) %>% round(2)
loocv_r2 <- calculate_r2(test_data$Strength, loocv_predictions) %>% round(2)

Result_Value <- c(loocv_rmse, loocv_mae, loocv_r2)
loocv_df <- data.frame(Measure_of_Error, Result_Value) 
loocv_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()
```


__________________________________________________


  - **Nested CV Results:**
  

```{r, warning=FALSE}
#| label: analysis_nest

set.seed(123)
outer_train_control <- trainControl(method = "cv", number = 5)
inner_train_control <- trainControl(method = "cv", number = 5)

nested_model <- function(data, indices) {
  train_data <- data[indices,]
  test_data <- data[-indices,]
  
  model <- train(Strength ~ Cement + Superplasticizer + Age + Water, data = train_data, method = "lm", trControl = inner_train_control)
  predictions <- predict(model, newdata = test_data)
  rmse <- calculate_rmse(test_data$Strength, predictions)
  mae <- calculate_mae(test_data$Strength, predictions)
  r2 <- calculate_r2(test_data$Strength, predictions)
  return(c(rmse, mae, r2))
}

nested_cv_indices <- createFolds(data_combined$Strength, k = 5, list = TRUE, returnTrain = TRUE)
nested_cv_results <- t(sapply(nested_cv_indices, nested_model, data = data_combined))
nested_cv_rmse <- mean(nested_cv_results[, 1]) %>% round(2)
nested_cv_mae <- mean(nested_cv_results[, 2]) %>% round(2)
nested_cv_r2 <- mean(nested_cv_results[, 3]) %>% round(2)

Result_Value <- c(nested_cv_rmse, nested_cv_mae, nested_cv_r2)
nested_cv_df <- data.frame(Measure_of_Error, Result_Value) 
nested_cv_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()

```

:::
::::

## LightGBM Model
:::: {layout="[[-20,60,-20], [100], [100]]"}
:::{.tweak90-text}

```{r, warning=FALSE}
#| label: analysis_lightgbm

#Here we are selecting the predictors and target variable. We combine them into a new dataframe. 
predictors <- data[, c("Cement", "Superplasticizer", "Age", "Water")]
target <- data$Strength
data_combined <- data.frame(predictors, Strength = target)

#Here we set a seed for reproducibility and create a partition index to split the data into 80/20. 
#We then split the dataframe into the training and test datasets based on the partition index. 
set.seed(123)
trainIndex <- createDataPartition(data_combined$Strength, p = .8, list = FALSE, times = 1)
train_data <- data_combined[trainIndex,]
test_data <- data_combined[-trainIndex,]

#Here we convert the training and testing data into matrix format for LightGBM.
#We also extract the target variable for both training and testing data. 
train_matrix <- as.matrix(train_data[, -ncol(train_data)])
train_label <- train_data$Strength
test_matrix <- as.matrix(test_data[, -ncol(test_data)])
test_label <- test_data$Strength

#Here we create a lightGBM dataset from the training data matrix and labels. 
dtrain <- lgb.Dataset(data = train_matrix, label = train_label)

#Here we define our paramaters for the LightGBM model.
params <- list(
  objective = "regression",
  metric = "rmse",
  learning_rate = 0.1,
  num_leaves = 31,
  max_depth = -1
)

#Here we train the lightGBM model with the specific paramaters and 100 rounds. 
model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1)


#Redefining evaluation functions
calculate_rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

calculate_mae <- function(actual, predicted) {
  mean(abs(actual - predicted))
}

calculate_r2 <- function(actual, predicted) {
  ss_total <- sum((actual - mean(actual))^2)
  ss_residual <- sum((actual - predicted)^2)
  r2 <- 1 - (ss_residual / ss_total)
  return(r2)
}

#K-Fold CV for LightGBM
set.seed(123)
folds <- createFolds(train_data$Strength, k = 5, list = TRUE)
kfold_rmse_values <- c()
kfold_mae_values <- c()
kfold_r2_values <- c()

for (i in 1:5) {
  fold_train <- train_data[folds[[i]], ]
  fold_test <- train_data[-folds[[i]], ]

  dtrain <- lgb.Dataset(data = as.matrix(fold_train[, -ncol(fold_train)]), label = fold_train$Strength)
  dtest <- as.matrix(fold_test[, -ncol(fold_test)])

  params <- list(objective = "regression", metric = "rmse", learning_rate = 0.1, num_leaves = 31, max_depth = -1)
  model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1)

  predictions <- predict(model, dtest)

  kfold_rmse_values[i] <- calculate_rmse(fold_test$Strength, predictions)
  kfold_mae_values[i] <- calculate_mae(fold_test$Strength, predictions)
  kfold_r2_values[i] <- calculate_r2(fold_test$Strength, predictions)
}

kfold_rmse <- mean(kfold_rmse_values) %>% round(2)
kfold_mae <- mean(kfold_mae_values) %>% round(2)
kfold_r2 <- mean(kfold_r2_values) %>% round(2)

Result_Value <- c(kfold_rmse, kfold_mae, kfold_r2)
kfold_lgbm_df <- data.frame(Measure_of_Error, Result_Value)
kfold_lgbm_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()
```


:::
_______________

:::{.tweak90-text}

  - Ensemble of decision trees
  - Uses gradient boosting
  - Final prediction is the sum of predictions from all individual trees
  - Feature importance

:::
::::



## LightGBM CV Results{.smaller}
::::{.tweak90-text}
::: {layout="[[30,70], [100], [30,70], [100], [30,70]]"}

  - **K-Fold Results:**

```{r, warning=FALSE}

kfold_lgbm_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()
```

________


  - **LOOCV Results:**

```{r, warning=FALSE}
#| label: analysis_lightgbm_loocv

set.seed(123)
train_control_loocv <- trainControl(method = "LOOCV")
loocv_predictions <- c()

for (i in 1:nrow(train_data)) {
  fold_train <- train_data[-i, ]
  fold_test <- train_data[i, , drop = FALSE]

  dtrain <- lgb.Dataset(data = as.matrix(fold_train[, -ncol(fold_train)]), label = fold_train$Strength)
  dtest <- as.matrix(fold_test[, -ncol(fold_test)])

  params <- list(objective = "regression", metric = "rmse", learning_rate = 0.1, num_leaves = 31, max_depth = -1)
  model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1) ############################################ Change nrounds = 100

  prediction <- predict(model, dtest)
  loocv_predictions <- c(loocv_predictions, prediction)
}

loocv_rmse <- calculate_rmse(train_data$Strength, loocv_predictions) %>% round(2)
loocv_mae <- calculate_mae(train_data$Strength, loocv_predictions) %>% round(2)
loocv_r2 <- calculate_r2(train_data$Strength, loocv_predictions) %>% round(2)

Result_Value <- c(loocv_rmse, loocv_mae, loocv_r2)
loocv_lgbm_df <- data.frame(Measure_of_Error, Result_Value)
loocv_lgbm_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()
```

________


  - **Nested CV Results:**

```{r, warning=FALSE}
#| label: analysis_lightgbm_nest

# LightGBM: Nested CV
set.seed(123)
outer_folds <- createFolds(data_combined$Strength, k = 5, list = TRUE)
nested_cv_rmse_values <- c()
nested_cv_mae_values <- c()
nested_cv_r2_values <- c()

for (i in 1:5) {
  outer_train <- data_combined[outer_folds[[i]], ]
  outer_test <- data_combined[-outer_folds[[i]], ]
  
  inner_folds <- createFolds(outer_train$Strength, k = 2, list = TRUE)
  inner_cv_rmse_values <- c()
  
  for (j in 1:2) {
    inner_train <- outer_train[inner_folds[[j]], ]
    inner_test <- outer_train[-inner_folds[[j]], ]
    
    dtrain <- lgb.Dataset(data = as.matrix(inner_train[, -ncol(inner_train)]), label = inner_train$Strength)
    dtest <- as.matrix(inner_test[, -ncol(inner_test)])
    
    params <- list(objective = "regression", metric = "rmse", learning_rate = 0.1, num_leaves = 31, max_depth = -1)
    model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1)
    
    predictions <- predict(model, dtest)
    inner_cv_rmse_values[j] <- calculate_rmse(inner_test$Strength, predictions)
  }
  
  best_inner_rmse <- min(inner_cv_rmse_values)
  
  dtrain <- lgb.Dataset(data = as.matrix(outer_train[, -ncol(outer_train)]), label = outer_train$Strength)
  dtest <- as.matrix(outer_test[, -ncol(outer_test)])
  
  model <- lgb.train(params = params, data = dtrain, nrounds = 100, verbose = -1)
  predictions <- predict(model, dtest)
  
  nested_cv_rmse_values[i] <- calculate_rmse(outer_test$Strength, predictions)
  nested_cv_mae_values[i] <- calculate_mae(outer_test$Strength, predictions)
  nested_cv_r2_values[i] <- calculate_r2(outer_test$Strength, predictions)
}

nested_cv_rmse <- mean(nested_cv_rmse_values) %>% round(2)
nested_cv_mae <- mean(nested_cv_mae_values) %>% round(2)
nested_cv_r2 <- mean(nested_cv_r2_values) %>% round(2)

Result_Value <- c(nested_cv_rmse, nested_cv_mae, nested_cv_r2)
nested_cv_lgbm_df <- data.frame(Measure_of_Error, Result_Value) 
nested_cv_lgbm_df %>%  
  rename(`Measure of Error`=1,Result=2) %>% 
  kable()
```

:::
::::

## Comparison of Models{.smaller}
:::: {layout="[30, 70]"}

::: {#first-column}
  - Performance Comparison:\
  \  Linear Regression vs. LightGBM
  - Advantages and disadvantages\
  \ of each model

:::

::: {#second-column}

```{r, warning=FALSE}
#| label: analysis_results_mlr

# Adding method to CV data frames
kfold_results  <- kfold_df %>% mutate(Method = "5-Fold") 
LOOCV_results  <- loocv_df %>% mutate(Method = "LOOCV")
nested_results <- nested_cv_df %>% mutate(Method = "Nested CV") 

# Combining 3 CV data frames.
# Result Summary Table
result_long_df <- combine(kfold_results, LOOCV_results, nested_results)

# Pivot wider tidy up
result_wide_df <- result_long_df %>% 
  pivot_wider(names_from = Measure_of_Error, values_from = Result_Value) %>% 
  arrange(desc(R2), RMSE, MAE)
# result_wide_df %>% kable()
```

<!-- ## Plot -->
<!-- ```{r, warning=FALSE} -->

<!-- # Summary Plot -->
<!-- summary_plot <- result_long_df %>% -->
<!--   ggplot(aes(x = Method, y = Result_Value)) + -->
<!--   geom_bar(stat="identity", color = "blue4", fill = "gray78") + -->
<!--   facet_wrap(~ Measure_of_Error, scales = "free", ncol = 3) + -->
<!--   labs(title = "Linnear Model", -->
<!--        y = "Mean Measure of Error", -->
<!--        x = "Cross-validation Method") + -->
<!--   theme(axis.title.x = element_text(face = "bold")) + -->
<!--   theme(axis.title.y = element_text(face = "bold")) + -->
<!--   theme(axis.title.title = element_text(face = "bold")) + -->
<!--   theme(strip.text = element_text(face = "bold")) -->

<!-- summary_plot -->
<!-- ``` -->


```{r, warning=FALSE}
#| label: analysis_results_LGMB

# Model Comparison Dataframe
Comparison_df <- rbind(
  Regression_R <- cbind(result_long_df, c(replicate(length(result_long_df),"Linear_Regression"))) %>% rename(Model=4),
  LightGBM_KFold_R <- kfold_lgbm_df %>% mutate(Method = "5-Fold", Model = "LightGBM"),
  LightGBM_KFold_R <- loocv_lgbm_df %>% mutate(Method = "LOOCV", Model = "LightGBM"),
  LightGBM_Nest_R  <- nested_cv_lgbm_df %>% mutate(Method = "Nested CV", Model = "LightGBM")
  ) 
  

# print Comparison_df
Comparison_df %>% 
  pivot_wider(names_from = Model, 
              values_from = Result_Value) %>% 
  relocate(Method) %>% 
  rename(`Measure of Error` = Measure_of_Error,
         `Linear Regression` = Linear_Regression) %>% 
  mutate(Method = if_else(Method == "Nested CV", "NCV", Method)) %>% 
  kable()
```
:::
::::


## Model Comparison K-Fold Plot
```{r, warning=FALSE}
# Model Comparison Plot: K-Fold
KFold_Plot <- Comparison_df %>%
  filter(Method == "5-Fold") %>%
  ggplot(aes(x=Model, y=Result_Value)) +
  geom_bar(stat="identity", color = "blue4", fill = "gray78") +
  facet_wrap(~ Measure_of_Error, scales = "free", ncol = 3) +
  labs(title = "Model Comparison",
       subtitle = "K-Fold Crosss-validation where K = 5",
       y = "Mean Measure of Error",
       x = "Model") +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.title = element_text(face = "bold")) +
  theme(strip.text = element_text(face = "bold"))
KFold_Plot
```

## Model Comparison LOOCV Plot
```{r, warning=FALSE}
# 
# print('Intentionally blank. Takes 10 minutes to render this code chunk \n Will turn it back on to finalize')

# Model Comparison Plot: LOOCV
LOOCV_Plot <- Comparison_df %>%
  filter(Method == "LOOCV") %>%
  ggplot(aes(x=Model, y=Result_Value)) +
  geom_bar(stat="identity", color = "blue4", fill = "gray78") +
  facet_wrap(~ Measure_of_Error, scales = "free", ncol = 3) +
  labs(title = "Model Comparison",
       subtitle = "Leave-one-out Cross-validation",
       y = "Mean Measure of Error",
       x = "Model") +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.title = element_text(face = "bold")) +
  theme(strip.text = element_text(face = "bold"))
LOOCV_Plot

```

## Model Comparison Nested CV Plot
```{r, warning=FALSE}
# Model Comparison Plot: Nested CV
Nested_CV_Plot <- Comparison_df %>%
  filter(Method == "Nested CV") %>%
  ggplot(aes(x=Model, y=Result_Value)) +
  geom_bar(stat="identity", color = "blue4", fill = "gray78") +
  facet_wrap(~ Measure_of_Error, scales = "free", ncol = 3) +
  labs(title = "Model Comparison",
       subtitle = "Nested Crosss-validation",
       y = "Mean Measure of Error",
       x = "Model") +
  theme(axis.title.x = element_text(face = "bold")) +
  theme(axis.title.y = element_text(face = "bold")) +
  theme(axis.title.title = element_text(face = "bold")) +
  theme(strip.text = element_text(face = "bold"))
Nested_CV_Plot
```






## LightGBM (Light Gradient Boosting Machine)

- **Description:** A gradient boosting framework that uses tree-based learning algorithms.
- **Pros:** High efficiency, fast training, and capable of handling large datasets.
- **Cons:** Requires careful tuning of parameters.



## Predictive Performance Comparison


  - LightGBM outperformed traditional models and cross-validation techniques.
  - Lower prediction errors and more reliable performance metrics.
  - Demonstrated strong generalization capabilities.


## Computational Efficiency

- **LightGBM:** Fast training and efficient computation.
- **Nested Cross-Validation:** Excellent performance but computationally intensive.
- Efficiency crucial for real-world applications with limited resources.



## Conclusion

  - Cross-validation techniques and LightGBM effectively reduce overfitting and enhance model accuracy.
  - LightGBM offers superior accuracy and efficiency.
  - Identified key predictors for accurate model development.
  - Robust framework for model evaluation, improving decision-making in concrete design and construction.


<!-- In this study, we analyzed cross-validation techniques that can be used for evaluating concrete strength modeling performance, including K-Fold, leave-one-out cross-validation, and Nested cross-validation. In our case, we were able to examine the linear regression performance of an entire data set and then compare it with the performance of cross-validation techniques. The findings pointed out that leave-one-out cross-validation, K-fold cross-validation, and nested cross-validation techniques had a better generalization error compared with conventional linear regression models. The detailed models established better results regarding the actual concrete strength. We can note that the nested cross-validation slightly performed better than the k-fold and leave-one-out cross-validation techniques. Further, the research also stressed the feature selection part, as variables like water-cement ratio, age, and type of aggregate were identified as main attributes influencing concrete strength. Overall, this kind of assessment raises the prospect of generating vehicle models for refining forecast precision and effectiveness, as well as investigating the best strategies for developing concrete mixes and promoting construction improvement. -->

## Future Research


  - Further refinement of these techniques to improve predictive accuracy.
  - Exploration of additional advanced models.
  - Application in various engineering contexts to enhance model reliability and performance.


## References

<span style="color:blue"> All figures were created by the authors. </span>